\begin{table}[!htbp]
\centering
\small
\begin{tabular}{lrrrr}
\toprule
\textbf{Dataset} & \textbf{\#Vectors} & \textbf{Dim.} & \textbf{Distance} & \textbf{\#Queries} \\
\midrule
SIFT        & 1{,}000{,}000{,}000 & 128 & L2     & 10{,}000 \\
SpaceV\footnotemark[1] & 100{,}000{,}000     & 100 & L2     & 29{,}316 \\
DEEP\footnotemark[1]   & 10{,}000{,}000      & 96  & L2     & 10{,}000 \\
GIST        & 1{,}000{,}000       & 960 & L2     & 1{,}000 \\
GloVe       & 1{,}000{,}000       & 100 & Cosine & 10{,}000 \\
\bottomrule
\end{tabular}
\caption{Dataset statistics}
\label{tab:datasets}
\end{table}

\footnotetext[1]{For \textsc{SpaceV} and \textsc{DEEP}, we use the first 100M and 10M vectors of the 1B vectors in respective datasets.}

\section{Experiments}

\subsection{Experimental Setup}
We implement QVCache in C++, together with Python bindings. All experiments are conducted on a Linux system in a containerized Docker environment, equipped with an Intel Xeon Gold 5118 processor, 2.30GHz, with 24 physical cores, 376GB of DDR4 RAM, and a Dell Express Flash PM1725a 1.6TB NVMe SSD.

\textbf{Datasets.} We evaluate how well QVCache generalizes across diverse data by benchmarking it on five datasets that differ in scale, domain, and dimensionality, as summarized in Table~\ref{tab:datasets} \cite{sift, deep, gist, pennington-etal-2014-glove, SPACEV1B_SPTAG}. 

\textbf{Backends.} We evaluate QVCache across a range of backend databases to understand its performance under diverse scenarios. We employ the DiskANN \cite{diskann} implementation by Yu et al. (2025) \cite{yu2025topologyawarelocalizedupdatestrategy}, a state-of-the-art disk-based vector search framework, for benchmarking QVCache across multiple datasets. Additionally, we test QVCache with FAISS \cite{faiss}, pgvector \cite{pgvector}, Qdrant \cite{qdrant2025}, Pinecone \cite{pinecone}, and SPANN \cite{spann} to assess its effectiveness with backends that differ in storage model (in-memory, disk-based, or hybrid), deployment model (on-premises vs. cloud-based), and index type (graph, tree, etc.).

\textbf{Metrics.} %We evaluate QVCache using six metrics, as illustrated in Figures \ref{fig:dataset-experiments} and \ref{fig:backend-experiments}. Metrics are collected at the granularity of window steps, with each dot representing a step. 
We evaluate QVCache across six metrics measured per window, with values reported at window-step granularity, where each point in the figures corresponds to a single step.
\emph{Cache hit ratio} measures the fraction of queries served by QVCache without forwarding requests to the backend database, while \emph{Hit latency} captures the latency of these queries. We use P50 latency and omit P99 latency because, unless QVCache achieves a hit ratio above 99\%, P99 is dominated by queries that miss the cache and are served by the backend, yielding no meaningful distinction when using QVCache. %Instead, we use P50 latency to evaluate overall latency performance. %, independent of cache hits or misses.
Although QVCache is primarily designed for low-latency responses, it also improves throughput; we report the metric reflecting this benefit. To measure query accuracy, we use 10-recall@10. We also track the number of vectors retrieved from the backend into the cache over time to assess eviction behavior. 

\begin{figure*}[t]
\centering
% Define the column width for 5 columns (Using 0.195\textwidth for slight height increase)
\newlength{\colwidth}
\setlength{\colwidth}{0.195\textwidth}

% Column Order: sift, spacev100m, deep10m, text2image, glove

% ===== Row 1: hit_ratio =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/hit_ratio.pdf}
\end{subfigure}
\\ % Add vertical space between rows

% ===== Row 2: avg_hit_latency (where the "shift" was observed) =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/avg_hit_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/avg_hit_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/avg_hit_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/avg_hit_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/avg_hit_latency.pdf}
\end{subfigure}
\\ % Add vertical space between rows

% ===== Row 3: p50_latency =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/p50_latency.pdf}
\end{subfigure}
\\ % Add vertical space between rows

% ===== Row 4: qps =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/qps.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/qps.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/qps.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/qps.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/qps.pdf}
\end{subfigure}
\\ % Add vertical space between rows

% ===== Row 5: recall =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/recall.pdf}
\end{subfigure}

% ===== Row 6: Memory Active Vectors =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/memory_active_vectors.pdf}
\end{subfigure}

% ===== Row 7: Column captions (Datasets) =====
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (a) \textbf{SIFT}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (b) \textbf{SpaceV}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (c) \textbf{DEEP}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (d) \textbf{GIST}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (e) \textbf{GloVe}
\end{subfigure}

\caption{Vector search performance of backend vector database (DiskANN) alone vs. backend augmented with QVCache on the five datasets. $k$ is set to 10.}

\label{fig:dataset-experiments}
\end{figure*}

\subsection{Adaptive Query-Aware Caching}
\label{sec:dataset-experiments}

A query-aware vector cache must adopt to non-stationary workloads, where the active working set drifts \cite{10.14778/2735461.2735465inmemoryperformanceforbigdata} over time. It should support varying dimensionalities, distance functions, and $k$ values while requiring minimal configuration (i.e., without manually tuning similarity thresholds), regardless of the underlying dataset.

%To evaluate these properties, we conduct experiments on five datasets, as shown in 
Figures \ref{fig:dataset-experiments} and \ref{fig:k-experiments} show the results. Workloads are generated with $N_{\text{split}} = 10$, $\eta = 0.01$, $N_{\text{repeat}} = 3$, and $stride = 1$. QVCache is configured with $\alpha = 0.9$, $n_{\text{buckets}} = 8$, $d_{\text{reduced}} = 16$, and $n_{\text{mini-index}} = 4$. The value of $D$ is set to 0.25 for SIFT, 0.001 for GloVe, and 0.075 for the remaining datasets. \texttt{ADAPTIVE} search strategy is used to scan mini-indexes.  The capacity of each mini-index, $c_{\text{mini-index}}$, is set per dataset to approximate the working set size, defined as the number of unique vectors appearing in query neighbor sets within a window. Hence, this determines the number of vectors to be retrieved by QVCache. 

Queries are processed by $t_{processing} = 24$ worker threads, while asynchronous tasks, including vector insertions and threshold learning, are handled by a separate pool of $t_{processing} = 16$ threads.

As shown in Figure~\ref{fig:dataset-experiments}, the hit ratios for datasets such as SpaceV and GIST stabilize around 0.75, while the remaining datasets reach a hit ratio of 1, representing ideal behavior, during the second and third window repetitions. Each decline in hit ratio corresponds to a window slide. Throughput and median p50 latency closely follow the hit ratio, with QVCache configurations achieving up to 50x higher throughput. Integrating QVCache with DiskANN reduces p50 latency by 40x to 300x compared to the DiskANN-only configuration.

Recall is minimally affected. SIFT, SpaceV, and DEEP show approximately 2\% drops, while GIST and GloVe reach 6–12\% at some points. This trade-off between hit ratio and recall can be tuned using the deviation factor $D$, as discussed in Section 6.5.

Cache hits achieve sub-millisecond latency on SIFT, SpaceV, and DEEP, approximately 0.2 to 0.4 milliseconds, with slightly higher latencies on other datasets, approximately 0.6 to 1.0 milliseconds. For GIST, the higher latency results from its 960-dimensional vectors, which increase computational cost. In GloVe, despite comparable mini-index capacity, hit latency is higher due to the sequential scanning strategy across all mini-indexes.

Finally, as shown in Figure \ref{fig:k-experiments}, QVCache adapts to queries with varying $k$ values without any additional configuration beyond the total cache capacity. Capacities of 6,000, 60,000, and 600,000 vectors were allocated for $k = 1$, 10, and 100, respectively, reflecting the linear increase in vectors retrieved per window as $k$ grows. Overall, performance trends remain consistent across different $k$ values, with latency improvements becoming more pronounced for larger $k$.


\begin{figure}[h]
    \centering

    % Top row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/k_experiments/all_comparison/hit_ratio.pdf}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/k_experiments/all_comparison/memory_active_vectors.pdf}
    \end{subfigure}

    \vspace{0.75em}

    % Bottom row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/k_experiments/all_comparison/recall.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/k_experiments/all_comparison/p50_latency.pdf}
    \end{subfigure}

    \caption{Effect of varying $k$ on SIFT dataset.}
    \label{fig:k-experiments}
\end{figure}

\subsection{Sensitivity to Cache Capacity and Mini Index Partitioning}

As first noted by Belady \cite{belady1966study}, an ideal cache would retain exactly the items that will be accessed again and evict only those that will never be referenced in the future. Such optimal behavior is attainable only with an unbounded cache or an oracle that perfectly predicts future accesses. Since neither assumption is practical, real systems are inherently constrained by finite capacity and imperfect eviction decisions. The same limitations apply to QVCache.

To assess how well QVCache maintains high hit ratios under fixed capacity, we repeat the experiment from Figure \ref{fig:dataset-experiments} on the SIFT dataset using identical parameters, except with $N_{\text{round}} = 2$, as shown in Figure \ref{fig:cache_size_experiments}. Perturbed copies of a base split within a window bring approximately 15,000 vectors into the cache. With a window size of 4, the working set therefore contains roughly 60,000 vectors. Since the stride is 1, the working set changes by about 25\% at each window slide. An ideal configuration for this workload would thus set $c_{\text{mini-index}} = 15{,}000$ and a total cache capacity of at least 60,000 vectors.

In practice, however, estimating working set sizes and shift rates can be difficult. We therefore examine QVCache’s sensitivity to total cache capacity and $c_{\text{mini-index}}$ in Figure \ref{fig:cache_size_experiments}. On the left, we fix $c_{\text{mini-index}}$ at 15,000 and vary the total cache capacity. When the capacity is sufficient to hold the working set, the hit ratio remains high and ideally converges to 1. When capacity is insufficient, QVCache exhibits the expected degradation in hit ratio, similar to conventional caches, while recall remains largely unaffected.

On the right, we fix the total cache capacity at 60,000 and vary $c_{\text{mini-index}}$. We find that $c_{\text{mini-index}}$ has little impact on overall hit ratio and recall. However, larger mini-indexes result in more information being evicted at once. If recently evicted vectors are accessed again shortly afterward, this can temporarily reduce the hit ratio. Consequently, partitioning the total capacity across multiple mini-indexes is generally preferable, although this introduces additional hit latency, as discussed in Section 6.4.

\begin{figure}[h]
\centering

% Two columns → ~0.48 of column width each
\setlength{\colwidth}{0.48\columnwidth}

% ===== Row 1: hit_ratio =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/total_cache_size_experiments/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/partitioning_experiments/hit_ratio.pdf}
\end{subfigure}
\\[0.5em]

% ===== Row 2: recall =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/total_cache_size_experiments/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/partitioning_experiments/recall.pdf}
\end{subfigure}
\\[0.5em]

% ===== Row 3: memory_active_vectors =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/total_cache_size_experiments/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/partitioning_experiments/memory_active_vectors.pdf}
\end{subfigure}
\\[0.8em]

% ===== Column Labels =====
\begin{subfigure}{\colwidth}
    \centering
    (a) Cache capacity
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    (b) Mini-index capacity
\end{subfigure}

\caption{
Effect of cache capacity and size of mini-indexes in QVCache on the SIFT dataset.
Left: varying total cache capacity via $n_{\text{mini-index}}$.
Right: varying $c_{\text{mini-index}}$ with fixed total capacity.}
\label{fig:cache_size_experiments}
\end{figure}


\begin{figure}[h]
    \centering

    \begin{subfigure}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/granularity_experiments/avg_hit_latency.pdf}
        \label{fig:plot1}
    \end{subfigure}

    \caption{Effect of mini-index granularity on QVCache performance. The total cache capacity is fixed at 50{,}000 vectors, while the number of mini-indexes is varied to control per–mini-index capacity.
}
    \label{fig:granularity-effect}
\end{figure}

\subsection{Granularity Matters: Balancing Eviction Cost and Hit Latency}

The granularity at which data is stored and retrieved has been extensively studied for decades. This includes classic work on page-size selection in database buffer pools and operating system caches~\cite{gray1998fiveminuteruleyearslater,7816668}, as well as more recent studies in cloud database systems, where access costs to remote storage layers are significantly higher~\cite{Zimmerer_2025}. These works explore different trade-offs, such as balancing I/O amortization against eviction cost and cache efficiency. For example, modern analytical systems like Snowflake~\cite{10.1145/2882903.2903741} store data in micropartitions to carefully balance parallelism, metadata overhead, and networked storage access costs. In traditional database systems, this problem is typically framed as finding an appropriate balance between the number of I/O operations required per record access and the effectiveness of the buffer cache.

Larger mini-indexes incur higher information loss upon eviction, as evicting a single mini-index removes a larger set of cached vectors at once. In the extreme case where $n_{\text{mini-index}} = 1$, the entire cache contents are evicted upon the first cache miss after the cache reaches full capacity. This motivates partitioning the cache into finer-grained units using multiple mini-indexes, thereby reducing eviction-induced information loss.

However, this increased granularity comes at a cost. Reducing the capacity of each mini-index can negatively impact cache-hit latency. The FreshVamana algorithm reduces the number of distance computations logarithmically with respect to the index size. Splitting a single FreshVamana index into multiple smaller mini-indexes diminishes this benefit. As a result, the cost of a cache-hit search can be approximated as
\[
C(N) \propto n_{\text{mini-index}} \cdot \log\!\left(\frac{N}{n_{\text{mini-index}}}\right),
\]
where $N$ denotes the total cache capacity in terms of vectors. Consequently, the cache cannot be arbitrarily partitioned into increasingly smaller mini-indexes, as the linear increase in the number of mini-indexes eventually outweighs the logarithmic reduction in per-index search cost.

To further analyze this trade-off, we conducted the experiment shown in Figure~\ref{fig:granularity-effect} using the SIFT dataset. We fixed the total cache capacity and varied the number of mini-indexes, $n_{\text{mini-index}}$, which implicitly determines the capacity of each mini-index, $c_{\text{mini-index}}$. All other parameters were kept identical to those described in Section~5.3. As $n_{\text{mini-index}}$ increases, the average cache-hit latency rises, since the scanning strategy must probe a larger number of mini-indexes before identifying a confident candidate neighbor set.

Based on our experimental results, QVCache capacity should not be scaled vertically by indefinitely increasing $c_{\text{mini-index}}$. Instead, horizontal scaling through the addition of multiple mini-indexes is preferable, as it reduces eviction-induced information loss. However, this partitioning should not be pushed beyond the point where an individual mini-index can no longer accommodate a working set, as overly fine-grained partitioning degrades cache-hit latency.


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/deviation_factor_experiments/hit_ratio.pdf}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/deviation_factor_experiments/recall.pdf}
    \end{subfigure}

    \caption{Deviation Factor Effect on Hit Ratio - Recall on SIFT}
    \label{fig:deviation-factor-experiment}
\end{figure}

\subsection{Controlling Recall and Cache Hit Ratio via Deviation Factor}

QVCache dynamically adjusts similarity thresholds during execution. In addition, it allows explicit control over the trade-off between recall and cache hit ratio through the deviation factor, $D$.

We conducted an experiment on the SIFT dataset in which all settings were kept identical to those described in Section 6.2, except that $D$ was varied as a controlled parameter. The results, shown in Figure~\ref{fig:deviation-factor-experiment}, demonstrate that increasing $D$ leads to a higher cache hit ratio at the cost of reduced recall. In practice, this trade-off exhibits a saturation effect: beyond a certain point, further increasing $D$ yields diminishing improvements in hit ratio and only marginal reductions in recall.

There is no universal rule for selecting an appropriate deviation factor, as it depends on multiple factors including the dataset, vector dimensionality, query distribution, and distance metric. We therefore recommend initializing $D$ with a conservative value and gradually increasing it while monitoring recall and hit-ratio metrics. Since QVCache does not maintain any internal state that depends on $D$, adjusting this parameter can be performed online without requiring system downtime.


\begin{figure}[h]
    \centering

    \begin{subfigure}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/spatial_threshold_experiments/recall.pdf}
        \label{fig:plot1}
    \end{subfigure}

    \caption{Impact of Global vs. Spatial Threshold(s) on Recall}
    \label{fig:spatial-threshold-experiment}
\end{figure}


\subsection{Spatial Thresholds Are Key to Correct Cache Hit/Miss Decisions}
Vector distributions vary significantly across the vector space: some regions are densely clustered, while others are sparse. This heterogeneity makes it impractical to rely on a single global similarity threshold for all cache hit decisions.

To evaluate this effect, we repeated the experiment from Section 5.2 on the SIFT dataset under two configurations: one using a single global threshold and the other using spatial thresholds. As shown in Figure~\ref{fig:spatial-threshold-experiment}, the global threshold aggregates updates from cache misses across all regions, failing to capture local patterns and causing up to a 15\% loss in recall due to incorrect hit/miss decisions. In contrast, spatial thresholds adapt to local variations in the query distribution, limiting recall degradation to at most 2–3\%, demonstrating their effectiveness in preserving the backend database’s accuracy.


\begin{figure}[h]
    \centering

    % Row 1: Hit Ratio
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/pca_experiments/d_reduced/hit_ratio.pdf}
        \subcaption{$d_{\text{reduced}}$ — Hit Ratio}
    \end{minipage}\hfill
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/pca_experiments/n_buckets/hit_ratio.pdf}
        \subcaption{$n_{\text{buckets}}$ — Hit Ratio}
    \end{minipage}

    \vspace{0.8em}

    % Row 2: Recall
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/pca_experiments/d_reduced/recall.pdf}
        \subcaption{$d_{\text{reduced}}$ — Recall}
    \end{minipage}\hfill
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/pca_experiments/n_buckets/recall.pdf}
        \subcaption{$n_{\text{buckets}}$ — Recall}
    \end{minipage}

    \caption{Impact of granularity of space partitioning and dimensionality reduction on recall and hit ratio. Left: varying $d_{\text{reduced}}$ (fixed $n_{\text{buckets}}$ to 8). Right: varying $n_{\text{buckets}}$ (fixed $d_{\text{reduced}}$ to 16).}

    \label{fig:pca_experiments}
\end{figure}


\subsection{Sensitivity Analysis: Space Partitioning and Dimensionality Reduction}

We evaluate QVCache’s sensitivity to the granularity of space partitioning ($n_{buckets}$) and dimensionality reduction ($d_{reduced}$) by repeating the experiment from Figure \ref{fig:dataset-experiments} on the GIST dataset with $WINDOW\_SIZE = 1$ and $stride = 1$. As shown in Figure \ref{fig:pca_experiments}, the left column fixes $n_{buckets}$ at 8 and varies $d_{reduced}$, while the right column fixes $d_{reduced}$ at 16 and varies $n_{buckets}$, reporting the resulting hit ratio and recall.

GIST vectors have 960 dimensions. Increasing $d_{reduced}$ to 128 or higher provides only a modest improvement in recall (around 3–4\%) while significantly reducing the hit ratio, highlighting the effectiveness of dimensionality reduction for guiding cache hit–miss decisions.

Similarly, increasing $n_{buckets}$ from 8 to 128 yields an average recall improvement of roughly 5\% but heavily reduces the hit ratio. This behavior arises because Algorithm \ref{alg:learn-threshold} overfits local patterns (i.e. the partitioning is so fine-grained that $\theta[k][R]$ learns almost a query-specific estimate of $\text{d}{_\text{backend}}[k]$ in each region) and fails to generalize across queries.

\begin{table}[h]
\centering
\captionsetup{skip=6pt}
\small % Reduces font size; use \footnotesize for even smaller
\renewcommand{\arraystretch}{1.0} % Reduced from 1.15
\setlength{\tabcolsep}{4pt}    % Reduced from 6pt

\begin{tabular}{|>{\centering\arraybackslash}m{2.2cm}|c|c|c|c|c|c|}
\hline
\diagbox[width=2.2cm,height=0.9cm]
{$c_{\text{mini-index}}$}{$n_{\text{mini-index}}$}
& 1 & 2 & 4 & 8 & 16 & 32 \\
\hline
3{,}125
& \cellcolor{gray!25}16
& \cellcolor{orange!25}24
& \cellcolor{yellow!25}37
& \cellcolor{green!25}62
& \cellcolor{blue!25}113
& \cellcolor{purple!25}219 \\
\hline
6{,}250
& \cellcolor{orange!25}18
& \cellcolor{yellow!25}34
& \cellcolor{green!25}56
& \cellcolor{blue!25}100
& \cellcolor{purple!25}189
& \\
\hline
12{,}500
& \cellcolor{yellow!25}23
& \cellcolor{green!25}58
& \cellcolor{blue!25}99
& \cellcolor{purple!25}183
& & \\
\hline
25{,}000
& \cellcolor{green!25}34
& \cellcolor{blue!25}108
& \cellcolor{purple!25}170
& & & \\
\hline
50{,}000
& \cellcolor{blue!25}56
& \cellcolor{purple!25}171
& & & & \\
\hline
100{,}000
& \cellcolor{purple!25}99
& & & & & \\
\hline
\end{tabular}

\caption{Memory usage (in MB) of QVCache with varying
$n_{\text{mini-index}}$ and $c_{\text{mini-index}}$ (in vectors) on SIFT.}
\label{tab:memory-footprint-experiment}
\end{table}

\subsection{Memory Overhead Analysis of QVCache}

QVCache maintains only the hottest vectors in its cache and evicts entries once the allocated capacity is reached, ensuring memory usage never exceeds the configured limit. To quantify this, we replicate the experiment from Section \ref{sec:dataset-experiments} under varying cache capacities by adjusting $n_{mini-index}$ and $c_{mini-index}$, as summarized in Table \ref{tab:memory-footprint-experiment}.

For a billion-scale dataset such as SIFT, DiskANN requires 33.5 GB of memory. Adding QVCache with a capacity of 100,000 vectors incurs only 100–200 MB of overhead. Memory usage grows linearly with total capacity (and naturally with vector dimensionality), and partitioning across multiple mini-indexes further increases overhead, as shown by the diagonals in Table \ref{tab:memory-footprint-experiment}.


For workloads with working sets around 100,000 vectors, the overhead remains around 200 MB, suitable for client-side caching. When embedding QVCache directly into backend systems, extreme cases with working sets up to 1 million vectors result in roughly 2 GB of overhead. Considering that disk-based systems require tens of gigabytes and in-memory systems require hundreds of gigabytes, this overhead is negligible compared to the latency and throughput improvements enabled by QVCache.

Moreover, the memory required to store distance thresholds is negligible and is included in the numbers reported in Table \ref{tab:memory-footprint-experiment}. For example, QVCache learns roughly 1.5K, 15K, and 50K thresholds for GIST, SIFT, and SpaceV in Figure \ref{fig:dataset-experiments}, consuming about 200 KB for SpaceV. Even under pessimistic assumptions where, e.g., one million thresholds are learned due to highly diverse queries or varying values of $k$, the footprint remains around 4 MB. Users may optionally cap the number of stored thresholds, evicting and relearning them if needed. Overall, threshold storage contributes insignificantly to QVCache’s memory use.

\begin{figure}[h]
    \centering

    % Top row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/noise_ratio_experiments/hit_ratio.pdf}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/noise_ratio_experiments/recall.pdf}
    \end{subfigure}

    \vspace{0.75em}

    % Bottom row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/noise_ratio_experiments/p50_latency.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/noise_ratio_experiments/memory_active_vectors.pdf}
    \end{subfigure}

    \caption{Effect of increasing noise ratio, $\eta$, on DEEP dataset.}
    \label{fig:noise-ratio-experiments}
\end{figure}

\subsection{Stress-Testing QVCache in the Absence of Temporal–Semantic Locality}

In Figure~\ref{fig:noise-ratio-experiments}, we evaluate how the noise ratio $\eta$ affects QVCache by repeating the experiment from Section~\ref{sec:dataset-experiments} while increasing $\eta$ up to~0.6. To avoid triggering evictions and simply observe how many vectors QVCache retrieves into the cache, we set the cache capacity to 1M vectors. As shown in Figure~\ref{fig:overlap-analysis}, $\eta = 0.6$ represents the extreme case in which perturbed queries share no overlap in their top-$k$ neighbors, i.e., every query effectively becomes unique. Even under this setting, QVCache sustains high hit ratios while degrading recall by less than~4\%.

This result reveals an important phenomenon: pairwise uniqueness among queries does not imply global dissimilarity across a workload. Although no two perturbed queries share top-$k$ neighbors, the collective set of vectors they reference still exhibits overlap at scale. This is reflected in the curves for $\eta = 0.4$ and $\eta = 0.6$, where the number of vectors inserted into the cache increases only modestly. Thus, QVCache may remain effective even when similar queries do not repeat, leveraging potential collaboration across many unique queries rather than relying solely on strict temporal-semantic locality.





\begin{figure*}[t]
\centering

% Adjusted column width for 5 columns (roughly 1/5 of text width)
\setlength{\colwidth}{0.18\textwidth}

% ===== Row 1: hit_ratio =====
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/faiss/hit_ratio.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/qdrant/hit_ratio.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/pgvector/hit_ratio.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/pinecone/hit_ratio.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/sptag/hit_ratio.pdf}
%\end{subfigure}

% ===== Row 2: avg_hit_latency =====
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/faiss/avg_hit_latency.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/qdrant/avg_hit_latency.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/pgvector/avg_hit_latency.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/pinecone/avg_hit_latency.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/sptag/avg_hit_latency.pdf}
%\end{subfigure}

% ===== Row 3: p50_latency =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/faiss/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/qdrant/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/pgvector/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/pinecone/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/sptag/p50_latency.pdf}
\end{subfigure}

% ===== Row 4: qps =====
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/faiss/qps.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/qdrant/qps.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/pgvector/qps.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/pinecone/qps.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/sptag/qps.pdf}
%\end{subfigure}

% ===== Row 5: recall =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/faiss/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/qdrant/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/pgvector/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/pinecone/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/sptag/recall.pdf}
\end{subfigure}

% ===== Row 6: memory_active_vectors =====
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/faiss/memory_active_vectors.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/qdrant/memory_active_vectors.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/pgvector/memory_active_vectors.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/pinecone/memory_active_vectors.pdf}
%\end{subfigure}
%\begin{subfigure}{\colwidth}
%    \includegraphics[width=\linewidth]{plots/backend_experiments/sptag/memory_active_vectors.pdf}
%\end{subfigure}

% ===== Row 7: Column captions =====
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(a) \textbf{FAISS}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(b) \textbf{Qdrant}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(c) \textbf{pgvector}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(d) \textbf{Pinecone}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(e) \textbf{SPANN}
\end{subfigure}

\caption{Performance of Various Backend Databases With and Without QVCache on DEEP Dataset}
\label{fig:backend-experiments}
\end{figure*}

\subsection{Backend-Agnostic Caching}

QVCache is compatible with any vector search system, independent of the underlying index type, system scale, or deployment environment, requiring only the implementation of standard search and fetch interfaces. To illustrate this property, we conduct the experiments shown in Figure \ref{fig:backend-experiments}, using the same parameter settings as in Section 6.2 for the DEEP dataset. All backends are evaluated both with and without QVCache. FAISS, Qdrant, and pgvector are deployed on the Linux host described in Section 6.2, while Pinecone is evaluated using its managed cloud service.

Pinecone \cite{pinecone}, a cloud-managed vector search service, exhibits relatively high latencies ($\approx$ 100 ms) due to network round-trip overheads. As shown in Figure \ref{fig:backend-experiments}, integrating QVCache on the client side bypasses this network latency and reduces p50 latency by up to three orders of magnitude ($\approx$ 1000×). Although cache-miss fetches may incur additional time, we observe no degradation in recall or hit-rate convergence. %Beyond latency reduction, QVCache also lowers serving costs for services billed on a per-query basis by converting a large fraction of requests into local, non-billable cache hits.

For hybrid memory–disk backends, such as Qdrant \cite{qdrant2025} and SPANN \cite{spann}, and disk-only backends like pgvector \cite{pgvector}, QVCache provides substantial latency improvements by fronting their client libraries. Specifically, we observe up to $\approx$100×, $\approx$300×, and $\approx$500× reductions in p50 latency for Qdrant, SPANN, and pgvector, respectively. While our experiments use client-side integration, embedding QVCache directly within these systems would enable cross-client caching, exploiting a global view of incoming queries and allowing multiple clients’ requests to be served more efficiently through better aggregation.

Even for fully in-memory backends such as FAISS, QVCache achieves up to 40× latency reduction. Here, both the backend and QVCache maintain indexes and vectors in memory, yet cache hits are faster because QVCache constrains the search space, allowing best-first search to converge in fewer steps. Nonetheless, QVCache introduces an in-cache probe for every request; when backend latency is already low and cache hit rates are limited, this overhead can be non-negligible.

In summary, augmenting a state-of-the-art in-memory backend such as FAISS with QVCache results in approximately $40\times$ lower p50 latency. For disk-based vector search systems, augmenting them with QVCache yields $\approx 100$–$\approx 500\times$ reductions in p50 latency, and for cloud-hosted systems such as Pinecone, the benefit is magnified to nearly $1000\times$ by eliminating network latency on cache hits.





