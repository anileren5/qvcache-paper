\section{Experiments}

In this section, we empirically investigate the following questions:

\begin{itemize}
    \item How well QVCache generalizes across datasets, query sizes ($k$), and workloads with varying degrees of temporal-semantic locality (Sections \ref{sec:dataset-experiments}, \ref{sec:temporal-semantic-locality-experiments}).
    
    \item What performance gains QVCache delivers when integrated with diverse backend systems (Sections \ref{sec:dataset-experiments}, \ref{sec:backend-agnostic-experiments}).

    \item How effective spatial thresholds are compared to a single global threshold (Section \ref{sec:spatial-thresholds}).
    
    \item How sensitive QVCache is to its hyperparameters 
    %(e.g., $D$, ${\text{d}}_{\text{reduced}}$, $n_{\text{mini-index}}$, $c_{\text{mini-index}}$)
    %and the impact on performance 
    (Sections \ref{sec:granularity-matters}, \ref{sec:cache-capacity-mini-index-partitioning},  \ref{sec:deviation-factor}, \ref{sec:space-partitioning}).
        
    \item What memory overhead QVCache incurs (Section \ref{sec:memory-overhead}).
\end{itemize}

\begin{table}[!htbp]
\centering
\small
\begin{tabular}{lrrrr}
\toprule
\textbf{Dataset} & \textbf{\#Vectors} & \textbf{Dim.} & \textbf{Distance} & \textbf{\#Queries} \\
\midrule
SIFT        & 1{,}000{,}000{,}000 & 128 & L2     & 10{,}000 \\
SpaceV\footnotemark[1] & 100{,}000{,}000     & 100 & L2     & 29{,}316 \\
DEEP\footnotemark[1]   & 10{,}000{,}000      & 96  & L2     & 10{,}000 \\
GIST        & 1{,}000{,}000       & 960 & L2     & 1{,}000 \\
GloVe       & 1{,}000{,}000       & 100 & Cosine & 10{,}000 \\
\bottomrule
\end{tabular}
\caption{Dataset statistics}
\label{tab:datasets}
\vspace{-\baselineskip}
\end{table}

\footnotetext[1]{For \textsc{SpaceV} and \textsc{DEEP}, we use the first 100M and 10M vectors of the 1B vectors in respective datasets.}

\subsection{Experimental Setup}
\label{sec:experimental-setup}
We implement QVCache in C++, together with Python bindings. All experiments are conducted on a Linux system in a containerized Docker environment, equipped with an Intel Xeon Gold 5118 processor, 2.30GHz, with 24 physical cores, 376GB of DDR4 RAM, and a Dell Express Flash PM1725a 1.6TB NVMe SSD.

\textbf{Datasets.} We evaluate how well QVCache generalizes across diverse data by benchmarking it on five datasets that differ in scale, domain, and dimensionality, as summarized in Table~\ref{tab:datasets} \cite{sift, deep, gist, pennington-etal-2014-glove, SPACEV1B_SPTAG}. 

\textbf{Backends.} We evaluate QVCache across a range of backend databases to understand its performance under diverse scenarios. We employ the DiskANN \cite{diskann} implementation by Yu et al. (2025) \cite{yu2025topologyawarelocalizedupdatestrategy}, a state-of-the-art disk-based vector search framework, for benchmarking QVCache across multiple datasets. Additionally, we test QVCache with FAISS \cite{faiss}, pgvector \cite{pgvector}, Qdrant \cite{qdrant2025}, Pinecone \cite{pinecone}, and SPANN \cite{spann} to assess its effectiveness with backends that differ in storage model (in-memory, disk-based, or hybrid), deployment model (on-premises vs. cloud-based), and index type (graph, tree, etc.).

\textbf{Metrics.} %We evaluate QVCache using six metrics, as illustrated in Figures \ref{fig:dataset-experiments} and \ref{fig:backend-experiments}. Metrics are collected at the granularity of window steps, with each dot representing a step. 
We evaluate QVCache across six metrics measured per window, with values reported at window-step granularity, where each point in the figures corresponds to a single step.
\emph{Cache hit ratio} measures the fraction of queries served by QVCache without forwarding requests to the backend database, while \emph{Hit latency} captures the latency of these queries. We use P50 latency and omit P99 latency because, unless QVCache achieves a hit ratio above 99\%, P99 is dominated by queries that miss the cache and are served by the backend, yielding no meaningful distinction when using QVCache. %Instead, we use P50 latency to evaluate overall latency performance. %, independent of cache hits or misses.
Although QVCache is primarily designed for low-latency responses, it also improves throughput; we report the metric reflecting this benefit. To measure query accuracy, we use 10-recall@10. We also track the number of vectors retrieved from the backend into the cache over time to assess eviction behavior. 

\begin{figure}[h]
    \centering

    % Top row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/k_experiments/all_comparison/hit_ratio.pdf}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/k_experiments/all_comparison/memory_active_vectors.pdf}
    \end{subfigure}

    \vspace{0.75em}

    % Bottom row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/k_experiments/all_comparison/recall.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/k_experiments/all_comparison/p50_latency.pdf}
    \end{subfigure}

    \caption{Effect of varying $k$ on the SIFT dataset. Cache capacity is 100{,}000 for $k = 10$ and is scaled linearly with $k$ (downscaled for $k = 1$ and upscaled for $k = 100$).}
    \label{fig:k-experiments}
\end{figure}



\begin{figure*}[t]
\centering
% Define the column width for 5 columns (Using 0.195\textwidth for slight height increase)
\newlength{\colwidth}
\setlength{\colwidth}{0.195\textwidth}

% ===== Row 1: hit_ratio =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/hit_ratio.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/hit_ratio.pdf}
\end{subfigure}
\\ % Add vertical space between rows

% ===== Row 2: avg_hit_latency (where the "shift" was observed) =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/avg_hit_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/avg_hit_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/avg_hit_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/avg_hit_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/avg_hit_latency.pdf}
\end{subfigure}
\\ % Add vertical space between rows

% ===== Row 3: p50_latency =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/p50_latency.pdf}
\end{subfigure}
\\ % Add vertical space between rows

% ===== Row 4: qps =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/qps.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/qps.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/qps.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/qps.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/qps.pdf}
\end{subfigure}
\\ % Add vertical space between rows

% ===== Row 5: recall =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/recall.pdf}
\end{subfigure}

% ===== Row 6: Memory Active Vectors =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/bigann/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/spacev1b_100m/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/deep10m/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/gist/memory_active_vectors.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/dataset_experiments/glove/memory_active_vectors.pdf}
\end{subfigure}

% ===== Row 7: Column captions (Datasets) =====
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (a) \textbf{SIFT}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (b) \textbf{SpaceV}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (c) \textbf{DEEP}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (d) \textbf{GIST}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.8em}
    (e) \textbf{GloVe}
\end{subfigure}

\caption{Vector search performance of backend vector database (DiskANN) alone vs. backend augmented with QVCache on the five datasets. $k$ is set to 10.}

\label{fig:dataset-experiments}
    \vspace{-\baselineskip}
\end{figure*}

\vspace*{-\baselineskip} 
\subsection{Adaptive Query-Aware Caching}
\label{sec:dataset-experiments}

A query-aware vector cache must adopt to non-stationary workloads, where the active working set drifts \cite{10.14778/2735461.2735465inmemoryperformanceforbigdata} over time. It should support varying dimensionalities, distance functions, and $k$ values while requiring minimal configuration (i.e., without manually tuning similarity thresholds), regardless of the underlying dataset.

The workloads in Figures \ref{fig:k-experiments} and \ref{fig:dataset-experiments} are generated with $N_{\text{split}} = 10$, $\eta = 0.01$, $N_{\text{repeat}} = 3$, $WINDOW\_SIZE = 4$, $stride = 1$, and $N_{\text{round}} = 1$ for the datasets in Table \ref{tab:datasets}, to empirically evaluate the query-awareness of QVCache.

QVCache is configured with $\alpha = 0.9$ (adaptivity rate), $n_{\text{buckets}} = 8$, and $d_{\text{reduced}} = 16$. The \texttt{ADAPTIVE} search strategy is applied. We set $D$ to 0.25 for SIFT and SpaceV, and to 0.075 for the remaining datasets. The cache capacity is fixed at 100{,}000 vectors and partitioned into $n_{\text{mini-index}} = 4$ mini-indexes, each with a capacity of 25{,}000.

In Figure~\ref{fig:dataset-experiments}, when the working set stays stable for three consecutive window steps (i.e., the window shifts vertically in Figure~\ref{fig:sliding-window}), the hit ratio steadily increases as QVCache fills its mini-indexes with vectors from the active working set. Every third step, the working set changes by approximately 25\% (corresponding to a diagonal slide in Figure~\ref{fig:sliding-window}), which leads to a matching $\approx 25\%$ drop in hit ratio. For example, with a stride of 2, this drop would be $\approx 50\%$. Throughput also exhibits a fluctuating pattern, driven by changes in the hit rate.

Queries that result in a cache hit have sub-millisecond latencies, typically between 0.1 and 1~ms. These high hit ratios, combined with sub-millisecond hit latencies, yield up to 60–300$\times$ lower p50 latency compared to using DiskANN only without QVCache.

Despite these significant performance gains, recall is only slightly impacted, dropping by 2–5\%. This impact can be further mitigated by tuning $D$, at the cost of some reduction in hit rate, as will be discussed in Section~\ref{sec:deviation-factor}.


\begin{figure*}[t]
\centering

% Adjusted column width for 5 columns (roughly 1/5 of text width)
\setlength{\colwidth}{0.18\textwidth}

% ===== Row 1: p50_latency =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/faiss/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/qdrant/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/pgvector/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/pinecone/p50_latency.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/sptag/p50_latency.pdf}
\end{subfigure}

% ===== Row 2: recall =====
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/faiss/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/qdrant/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/pgvector/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/pinecone/recall.pdf}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \includegraphics[width=\linewidth]{plots/backend_experiments/sptag/recall.pdf}
\end{subfigure}

% ===== Row 3: Column captions =====
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(a) \textbf{FAISS}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(b) \textbf{Qdrant}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(c) \textbf{pgvector}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(d) \textbf{Pinecone}
\end{subfigure}
\begin{subfigure}{\colwidth}
    \centering
    \vspace{0.5em}
    \hspace{2em}(e) \textbf{SPANN}
\end{subfigure}

\caption{Performance of Various Backend Databases With and Without QVCache on DEEP Dataset}
\label{fig:backend-experiments}
    \vspace{-\baselineskip}
\end{figure*}

Moreover, QVCache adapts to varying $k$ values, as shown in Figure~\ref{fig:k-experiments}. Across all $k$ values, QVCache preserves high recall and hit ratio, while latency reductions become more pronounced with larger $k$, reaching up to 950$\times$ lower for $k = 100$.

\subsection{Evaluating QVCache with Different Backends}
\label{sec:backend-agnostic-experiments}
QVCache is compatible with any vector search system (i.e. backend-agnostic), independent of the underlying index type, system scale, or deployment environment, requiring only the implementation of standard search and fetch interfaces. To quantify this property, we repeat the experiment from Section~\ref{sec:dataset-experiments} on DEEP with different backends, as shown in Figure~\ref{fig:backend-experiments}. All backends are evaluated both with and without QVCache. FAISS, Qdrant, SPANN and pgvector are deployed on the same Linux host, while Pinecone is evaluated using its managed cloud service.

Pinecone \cite{pinecone}, a cloud-managed vector search service, exhibits relatively high latencies ($\approx$ 100 ms) due to network round-trip overheads. As shown in Figure \ref{fig:backend-experiments}, integrating QVCache on the client side bypasses this network latency and reduces p50 latency by up to three orders of magnitude ($\approx$ 1000$\times$). Although cache-miss fetches may incur additional time, we observe no degradation in recall or hit-rate convergence. %Beyond latency reduction, QVCache also lowers serving costs for services billed on a per-query basis by converting a large fraction of requests into local, non-billable cache hits.

For hybrid memory–disk backends, such as Qdrant \cite{qdrant2025} and SPANN \cite{spann}, and disk-only backends like pgvector \cite{pgvector}, QVCache provides substantial latency improvements by fronting their client libraries. Specifically, we observe up to $\approx$100$\times$, $\approx$300$\times$, and $\approx$500$\times$ reductions in p50 latency for Qdrant, SPANN, and pgvector, respectively. While our experiments use client-side integration, embedding QVCache directly within these systems would enable cross-client caching, exploiting a global view of incoming queries and allowing multiple clients’ requests to be served more efficiently through better aggregation, which would be an interesting future work.

Even for fully in-memory backends such as FAISS, QVCache achieves up to 40$\times$ latency reduction. Here, both the backend and QVCache maintain indexes and vectors in memory, yet cache hits are faster because QVCache constrains the search space, allowing best-first search to converge in fewer steps. %Nonetheless, QVCache introduces an in-cache probe for every request; when backend latency is already low and cache hit rates are limited, this overhead can be non-negligible.

%In summary, augmenting a state-of-the-art in-memory backend such as FAISS with QVCache results in approximately $40$$\times$ lower p50 latency. For disk-based vector search systems, augmenting them with QVCache yields $\approx 100$–$ 500$$\times$ reductions in p50 latency, and for cloud-hosted systems such as Pinecone, the benefit is magnified to nearly $1000$$\times$ by eliminating network latency on cache hits.

\begin{figure}[t]
    \centering

    \begin{subfigure}{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/spatial_threshold_experiments/recall.pdf}
        \label{fig:plot1}
    \end{subfigure}

    \caption{Impact of Global vs. Spatial Threshold(s) on Recall}
    \label{fig:spatial-threshold-experiment}
    \vspace{-\baselineskip}
\end{figure}

%\subsection{Spatial Thresholds Are Key to Correct Cache Hit/Miss Decisions}
\subsection{Spatial Thresholds vs. Global Threshold}
\label{sec:spatial-thresholds}
Vector distributions vary significantly across the vector space: some regions are densely clustered, while others are sparse. This heterogeneity makes it impractical to rely on a single global similarity threshold for all cache hit decisions.

To evaluate this effect, we repeated the experiment from Section \ref{sec:dataset-experiments} on the SIFT dataset under two configurations: one using a single global threshold and the other using spatial thresholds. As shown in Figure~\ref{fig:spatial-threshold-experiment}, using spatial thresholds preserves recall with at most a 2–3\% drop, whereas a single global threshold can incur losses of up to 16\%. The underlying reason is that spatial thresholds learn locally appropriate hit/miss sensitivities, while a single global threshold fails to capture local variations and thus degrades recall.


\subsection{Granularity Matters: Balancing Eviction Cost and Hit Latency}
\label{sec:granularity-matters}
As discussed in Section~\ref{sec:mini-indexes}, given a fixed cache capacity, we can reduce eviction-induced information loss by partitioning the cache across multiple mini-indexes. However, increasing the number of mini-indexes worsens cache lookup and consequently hit latencies, as indicated by the cost expression in Equation~\ref{exp:cache-search-cost}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/granularity_experiments/avg_hit_latency.pdf}
        \label{fig:avg-hit-latency}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/granularity_experiments/memory_active_vectors.pdf}
        \label{fig:memory-active-vectors}
    \end{subfigure}

    \caption{Effect of mini-index granularity on QVCache performance on the SpaceV dataset. The total cache capacity is fixed at 100{,}000 vectors, while the number of mini-indexes is varied to control per–mini-index capacity.}
    \label{fig:granularity-effect}
\end{figure}

To further analyze this trade-off, we conducted the experiment shown in Figure~\ref{fig:granularity-effect} using the SpaceV dataset. We fixed the total cache capacity and varied the number of mini-indexes, $n_{\text{mini-index}}$, which implicitly determines the capacity of each mini-index, $c_{\text{mini-index}}$. As $n_{\text{mini-index}}$ increases, the average cache-hit latency rises, since the scanning strategy must probe a larger number of mini-indexes before identifying a confident candidate neighbor set (Equation \ref{exp:cache-search-cost}). Conversely, eviction cost increases as $n_{\text{mini-index}}$ decreases, as evidenced by the sharp drops in cache vectors for $n_{\text{mini-index}}=1$ and $n_{\text{mini-index}}=2$ in Figure~\ref{fig:granularity-effect}, indicating large, bursty eviction events and increased eviction-induced information loss.




%For scaling the cache capacity, we recommend first estimating the working set size, $w$, of the workload and setting the capacity of each mini-index to this value, that is, $c_{\text{mini-index}} = w$. This choice allows the \texttt{EAGER} strategy to terminate after scanning only the first one or two mini-indexes in most cases. Therefore, it avoids over-partitioning by eliminating the linear dependence on $n_{\text{mini-index}}$ in Equation~\ref{exp:cache-search-cost}, replacing it with an approximately constant multiplier $c \approx 1\text{--}2$. The total cache capacity can then be scaled to $N$ vectors by adding additional mini-indexes of size $w$. As a result, the cache lookup cost becomes nearly constant with respect to the cache size $N$ and is well-approximated by $c \log w$. Moreover, as the working set changes, vectors from previous working sets become cold and are naturally evicted at the granularity of mini-indexes, which further reduces eviction-induced information loss.


\begin{figure}[h]
\centering
\setlength{\colwidth}{0.48\columnwidth}

% ===== Column 1 =====
\begin{minipage}[t]{\colwidth}
    \centering
    \includegraphics[width=\linewidth]{plots/total_cache_size_experiments/hit_ratio.pdf}\\[0.5em]
    \includegraphics[width=\linewidth]{plots/total_cache_size_experiments/recall.pdf}\\[0.5em]
    \includegraphics[width=\linewidth]{plots/total_cache_size_experiments/memory_active_vectors.pdf}\\[0.5em]
    \vfill
    \textbf{(a) Varying Cache Capacity}
    \label{fig:cache_size_experiments:a}
\end{minipage}
\hfill
% ===== Column 2 =====
\begin{minipage}[t]{\colwidth}
    \centering
    \includegraphics[width=\linewidth]{plots/partitioning_experiments/hit_ratio.pdf}\\[0.5em]
    \includegraphics[width=\linewidth]{plots/partitioning_experiments/recall.pdf}\\[0.5em]
    \includegraphics[width=\linewidth]{plots/partitioning_experiments/memory_active_vectors.pdf}\\[0.5em]
    \vfill
    \textbf{(b) Varying Mini-index capacity}
    \label{fig:cache_size_experiments:b}
\end{minipage}

\caption{
Effect of cache capacity and size of mini-indexes in QVCache on the SIFT dataset.
Left: varying total cache capacity via $n_{\text{mini-index}}$ with fixed $c_{\text{mini-index}}$.
Right: varying $c_{\text{mini-index}}$ with fixed total capacity.
}
\label{fig:cache_size_experiments}
\vspace{-\baselineskip}
\end{figure}

\subsection{Sensitivity to Cache Capacity and Mini Index Partitioning}
\label{sec:cache-capacity-mini-index-partitioning}
%As first noted by Belady~\cite{belady1966study}, an ideal cache would retain exactly the items that will be accessed again and evict only those that will never be referenced in the future. Such optimal behavior is attainable only with an unbounded cache or with an oracle that perfectly predicts future accesses. Since neither assumption is practical, real systems are inherently constrained by finite capacity and imperfect eviction decisions. The same limitations apply to QVCache.

To see how well QVCache captures cache hits under varying cache capacities and varying mini-index sizes, we repeat the experiment from Figure~\ref{fig:dataset-experiments} on the SIFT dataset with $N_{\text{round}} = 2$ (to evaluate how well it detects cache hits when revisiting the same window with newly perturbed queries), as shown in Figure~\ref{fig:cache_size_experiments}.

For the generated workload we have $4$ perturbed copies ($C_{i,j}$'s in Figure \ref{fig:evaluation-framework}) of each split $S_i$ since $WINDOW\_SIZE = 4$. Each perturbed copy of a split, brings approximately $15{,}000$ vectors into the cache. Therefore, the working set size of the workload, $w$, becomes $60{,}000$. Since $\textit{stride} = 1$, approximately one quarter of the working set changes at each window slide (every $N_{repeat} =3$ window steps).

Similar to any other cache, if its capacity, $N$, is not large enough to fit the working set, the hit ratio drops due to frequent evictions. We observe the same effect for QVCache in Figure ~\ref{fig:cache_size_experiments}a, where the cache capacity is varied while the mini-index capacity $c_{\text{mini-index}}$ is fixed at $15{,}000$. We see that when the cache capacity is smaller than the working set size, the hit ratios (red and blue lines) drop severely, as expected, whereas they remain high when the capacity is greater than or equal to the working set size (green and orange lines), also as expected. Moreover, while $N = 60{,}000$ shows a drop in hit ratio when the second round starts (at window step $21$), it stays constant at $1$ for $N = 120{,}000$, because the $N = 120{,}000$ setting is able to keep the vectors fetched from the first round in the cache, whereas the $N = 60{,}000$ setting has already evicted them and therefore has to bring them into memory again. Although we observe hit-ratio drops in the insufficient-capacity settings, recall remains unaffected and is in fact higher, since the majority of queries are then answered directly by the backend database.

We then fixed the total cache capacity at $N = 60{,}000$, which is just sufficient to hold the working set, and varied the mini-index capacity $c_{\text{mini-index}}$. In Figure~\ref{fig:cache_size_experiments}b, we see that QVCache is not sensitive to $c_{\text{mini-index}}$ in terms of hit ratio and recall, with the exception that eviction-induced information loss increases as $c_{\text{mini-index}}$ grows. %However, the number of vectors stored in the cache (red line) drops (eviction-induced information loss; see Section~\ref{sec:mini-indexes}) much more sharply as $c_{\text{mini-index}}$ increases.

Therefore, we recommend setting the total cache capacity $N$ (in vectors) large enough to accommodate the expected working set size, as is standard practice for any caching system, and generally larger. Due to the access skew described in Section~\ref{sec:workload-characteristics}, this is relatively inexpensive to provision. The working set size grows linearly with $k$ and with the number of diverse (semantically dissimilar) queries arriving within a reuse interval.

For partitioning the cache capacity, we recommend first estimating the working set size, $w$, of the workload and setting the capacity of each mini-index to this value, that is, $c_{\text{mini-index}} = w$. This choice allows the \texttt{EAGER} strategy to terminate after scanning only the first one or two mini-indexes in most cases. Therefore, it avoids over-partitioning by eliminating the linear dependence on $n_{\text{mini-index}}$ in Equation~\ref{exp:cache-search-cost}, replacing it with an approximately constant multiplier $c \approx 1\text{--}2$. The total cache capacity can then be scaled to $N$ vectors by adding additional mini-indexes of size $w$. As a result, the cache lookup cost becomes nearly constant with respect to the cache size $N$ and is well-approximated by $c \log w$. Moreover, as the working set changes, vectors from previous working sets become cold and are naturally evicted at the granularity of mini-indexes, which further reduces eviction-induced information loss.


%In summary, we recommend setting the total cache capacity $N$ (in vectors) large enough to accommodate the expected working set size, as is standard practice for any caching system, and generally larger. Due to the access skew described in Section~\ref{sec:workload-characteristics}, this is relatively inexpensive to provision. The working set size grows linearly with $k$ and with the number of diverse (semantically dissimilar) queries arriving within a caching interval (i.e., queries per window in Figure~\ref{fig:sliding-window}). A best-effort estimate of the working set can guide the choice of $c_{\text{mini-index}}$, as suggested in Section~\ref{sec:granularity-matters}. Even if this estimate is imperfect, recall and hit ratio remain largely unaffected (Figure~\ref{fig:cache_size_experiments}b), though hit latencies may increase.





\begin{figure}[h]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/deviation_factor_experiments/hit_ratio.pdf}
    \end{subfigure}\hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/deviation_factor_experiments/recall.pdf}
    \end{subfigure}

    \caption{Deviation Factor Effect on Hit Ratio - Recall on SIFT}
    \label{fig:deviation-factor-experiment}
    \vspace{-\baselineskip}
\end{figure}

\subsection{Controlling Recall and Cache Hit Ratio via Deviation Factor}
\label{sec:deviation-factor}
The hyperparameter $D$, the deviation factor, gives users explicit control over the trade-off between recall and hit ratio. To quantify its effect, we repeat the experiment from Section~\ref{sec:dataset-experiments} on SIFT while varying $D$. The results in Figure~\ref{fig:deviation-factor-experiment} show that increasing $D$ improves the cache hit ratio at the cost of reduced recall. In practice, this trade-off exhibits a saturation effect: beyond a certain point, further increases in $D$ yield only marginal gains in hit ratio while incurring only small additional recall loss.

We do not prescribe a single rule of thumb for choosing $D$. In our experiments in Section~\ref{sec:dataset-experiments}, we selected $D$ by starting from $0$ and incrementally increasing it by $0.025$ at each step, ensuring that the backend’s recall was maintained and monitoring the resulting hit ratios. Once the hit ratio stopped improving meaningfully, we stopped increasing $D$. This procedure can be performed online at runtime without any downtime.


%As shown in Figure~\ref{fig:spatial-threshold-experiment}, the global threshold aggregates updates from cache misses across all regions, failing to capture local patterns and causing up to a 15\% loss in recall due to incorrect hit/miss decisions. In contrast, spatial thresholds adapt to local variations in the query distribution, limiting recall degradation to at most 2–3\%, demonstrating their effectiveness in preserving the backend database’s accuracy.


\begin{figure}[h]
    \centering

    % Row 1: Hit Ratio
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/pca_experiments/d_reduced/hit_ratio.pdf}
        \subcaption{$d_{\text{reduced}}$ — Hit Ratio}
    \end{minipage}\hfill
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/pca_experiments/n_buckets/hit_ratio.pdf}
        \subcaption{$n_{\text{buckets}}$ — Hit Ratio}
    \end{minipage}

    \vspace{0.8em}

    % Row 2: Recall
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/pca_experiments/d_reduced/recall.pdf}
        \subcaption{$d_{\text{reduced}}$ — Recall}
    \end{minipage}\hfill
    \begin{minipage}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/pca_experiments/n_buckets/recall.pdf}
        \subcaption{$n_{\text{buckets}}$ — Recall}
    \end{minipage}

    \caption{Impact of granularity of space partitioning and dimensionality reduction on recall and hit ratio. Left: varying $d_{\text{reduced}}$ (fixed $n_{\text{buckets}}$ to 8). Right: varying $n_{\text{buckets}}$ (fixed $d_{\text{reduced}}$ to 16).}

    \label{fig:pca_experiments}
    \vspace{-\baselineskip}
\end{figure}

\subsection{Sensitivity Analysis: Space Partitioning and Dimensionality Reduction}
\label{sec:space-partitioning}

We evaluate QVCache’s sensitivity to the granularity of space partitioning ($n_{\text{buckets}}$) and dimensionality reduction ($d_{\text{reduced}}$) by repeating the experiment from Figure \ref{fig:dataset-experiments} on the GIST dataset with $WINDOW\_SIZE = 1$. As shown in Figure \ref{fig:pca_experiments}, the left column fixes ($n_{\text{buckets}}$) at 8 and varies ($d_{\text{reduced}}$), while the right column fixes ($d_{\text{reduced}}$) at 16 and varies ($n_{\text{buckets}}$), reporting the resulting hit ratio and recall.

GIST vectors have 960 dimensions. Increasing ($d_{\text{reduced}}$) to 128 or higher provides only a modest improvement in recall (around 3–4\%) while significantly reducing the hit ratio, highlighting the effectiveness of dimensionality reduction for guiding cache hit–miss decisions.

Similarly, increasing ($n_{\text{buckets}}$) from 8 to 128 yields an average recall improvement of roughly 5\% but heavily reduces the hit ratio. This behavior arises because Algorithm \ref{alg:learn-threshold} overfits local patterns (i.e. the partitioning is so fine-grained that $\theta[k][R]$ learns almost a query-specific estimate of $\text{d}{_\text{backend}}[k]$ in each region) and fails to generalize across queries.

\begin{table}[h]
\centering
\captionsetup{skip=6pt}
\small % Reduces font size; use \footnotesize for even smaller
\renewcommand{\arraystretch}{1.0} % Reduced from 1.15
\setlength{\tabcolsep}{4pt}    % Reduced from 6pt

\begin{tabular}{|>{\centering\arraybackslash}m{2.2cm}|c|c|c|c|c|c|}
\hline
\diagbox[width=2.2cm,height=0.9cm]
{$c_{\text{mini-index}}$}{$n_{\text{mini-index}}$}
& 1 & 2 & 4 & 8 & 16 & 32 \\
\hline
3{,}125
& \cellcolor{gray!25}16
& \cellcolor{orange!25}24
& \cellcolor{yellow!25}37
& \cellcolor{green!25}62
& \cellcolor{blue!25}113
& \cellcolor{purple!25}219 \\
\hline
6{,}250
& \cellcolor{orange!25}18
& \cellcolor{yellow!25}34
& \cellcolor{green!25}56
& \cellcolor{blue!25}100
& \cellcolor{purple!25}189
& \\
\hline
12{,}500
& \cellcolor{yellow!25}23
& \cellcolor{green!25}58
& \cellcolor{blue!25}99
& \cellcolor{purple!25}183
& & \\
\hline
25{,}000
& \cellcolor{green!25}34
& \cellcolor{blue!25}108
& \cellcolor{purple!25}170
& & & \\
\hline
50{,}000
& \cellcolor{blue!25}56
& \cellcolor{purple!25}171
& & & & \\
\hline
100{,}000
& \cellcolor{purple!25}99
& & & & & \\
\hline
\end{tabular}

\caption{Memory usage (in MB) of QVCache with varying
$n_{\text{mini-index}}$ and $c_{\text{mini-index}}$ (in vectors) on SIFT.}
\label{tab:memory-footprint-experiment}
\vspace{-\baselineskip}
\end{table}

\subsection{Memory Overhead Analysis of QVCache}
\label{sec:memory-overhead}

For a billion-scale dataset such as SIFT, DiskANN requires 33.5GB of memory, and in-memory backends like FAISS can reach into the hundreds of gigabytes. By comparison, adding QVCache with a capacity of 100{,}000 vectors incurs only 100–200MB of additional memory. As expected, memory usage grows linearly with total cache capacity (and with vector dimensionality), and we observe that partitioning across multiple mini-indexes further increases overhead, as indicated by the diagonals in Table~\ref{tab:memory-footprint-experiment}. However, this overhead remains negligible relative to the memory consumed by the backends. Even with a very generous QVCache budget (e.g., 1M vectors), the additional cost for SIFT is only about 1–2GB.

The memory required to store distance thresholds is also negligible and is already included in the numbers reported in Table~\ref{tab:memory-footprint-experiment}. For example, in Figure~\ref{fig:dataset-experiments}, QVCache learns roughly 1.5K, 15K, and 50K thresholds for GIST, SIFT, and SpaceV, respectively, consuming about 200KB for SpaceV. Even under pessimistic assumptions where, for instance, one million thresholds are learned due to highly diverse queries or varying values of $k$, the footprint remains around 4MB. Users may optionally cap the number of stored thresholds, evicting and relearning them if needed. Overall, threshold storage contributes insignificantly to QVCache’s memory usage.

\begin{figure}[h]
    \centering

    % Top row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/noise_ratio_experiments/hit_ratio.pdf}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/noise_ratio_experiments/recall.pdf}
    \end{subfigure}

    \vspace{0.75em}

    % Bottom row
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/noise_ratio_experiments/p50_latency.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{plots/noise_ratio_experiments/memory_active_vectors.pdf}
    \end{subfigure}

    \caption{Effect of increasing query noise ratio, $\eta$, on DEEP dataset.}
    \label{fig:noise-ratio-experiments}
    \vspace{-\baselineskip}
\end{figure}

\subsection{Stress-Testing QVCache in the Absence of Temporal–Semantic Locality}
\label{sec:temporal-semantic-locality-experiments}

We next study the robustness of QVCache to different degrees of query perturbation~$\eta$. In Figure~\ref{fig:noise-ratio-experiments}, we evaluate the effect of~$\eta$ by repeating the experiment from Section~\ref{sec:dataset-experiments} while increasing $\eta$ up to~0.6. To avoid triggering evictions and focus solely on how many vectors QVCache retrieves into the cache, we set the cache capacity to $1$M vectors. As shown in Figure~\ref{fig:overlap-analysis}, $\eta = 0.6$ represents an extreme case in which perturbed queries share no overlap in their top-$k$ neighbors. Even under this setting, QVCache sustains high achieve ratios while degrading recall by less than~4\%.

This result reveals an important phenomenon: pairwise dissimilarity among queries does not imply global dissimilarity across a workload. Although no two perturbed queries share top-$k$ neighbors, the collective set of vectors they reference still exhibits overlap at scale , i.e. under high query concurrency and volume. This is reflected in the curves for $\eta = 0.4$ and $\eta = 0.6$, where the number of vectors inserted into the cache increases only modestly. Thus, QVCache may remain effective even when similar queries do not repeat, leveraging collaboration across many unique queries rather than relying solely on strict temporal–semantic locality.





