\begin{abstract}
Vector databases have become a cornerstone of modern information retrieval, powering applications in recommendation, search, and retrieval-augmented generation (RAG) pipelines. While caching is a well-studied problem in traditional databases, a dedicated caching layer is absent in the vector search literature. Disk-based vector search suffers from high latencies in high-accuracy settings, whereas fully in-memory solutions cannot scale to billion-scale datasets. We present QVCache, the first backend-agnostic vector search cache designed to fill this gap, capable of working with any existing vector database. QVCache delivers sub-millisecond query latencies, achieving up to 300$\times$ lower latency on a cache hit compared to disk-based vector databases for billion-scale datasets. It uses only a megabyte-scale memory footprint by keeping in memory a very small hot set of vectors (approximately 0.1\% of the dataset) regardless of dataset size, which is negligible compared to fully in-memory systems for practical workloads. Unlike conventional caches, QVCache addresses the unique challenge of similarity caching, where cache hits cannot be determined by exact key matches. It solves this problem by applying an online learning algorithm that dynamically adjusts spatial distance thresholds governing cache hit decisions and keeps the cache size bounded through an eviction policy. QVCache is particularly effective for workloads exhibiting temporal-semantic locality, achieving orders of magnitude lower latencies on cache hits while maintaining recall comparable to that of the backend database. Our results establish QVCache as a missing but crucial caching layer for efficient large-scale similarity search.
\end{abstract}