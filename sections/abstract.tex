\begin{abstract}
Vector databases have become a cornerstone of modern information retrieval, powering applications in recommendation, search, and retrieval-augmented generation (RAG) pipelines. However, scaling Approximate Nearest Neighbor (ANN) search to high recall under strict latency SLOs remains fundamentally constrained by memory capacity and I/O bandwidth. Disk-based vector search systems suffer severe latency degradation at high accuracy, while fully in-memory solutions incur prohibitive memory costs at billion-scale. Despite the central role of caching in traditional databases, vector search lacks a general query-level caching layer capable of amortizing repeated query work.

We present QVCache, the first backend-agnostic query-level caching system for ANN search with bounded memory footprint. QVCache exploits semantic query repetition by performing similarity-aware caching rather than exact-match lookup. It dynamically learns region-specific distance thresholds using an online learning algorithm, enabling recall-preserving cache hits while bounding lookup latency and memory usage independently of dataset size. QVCache operates as a drop-in layer for existing vector databases. It maintains a megabyte-scale memory footprint and achieves sub-millisecond cache-hit latency, reducing end-to-end query latency by up to 40–1000× when integrated with existing ANN systems. For workloads exhibiting temporal-semantic locality, QVCache substantially reduces latency while preserving recall comparable to the underlying ANN backend, establishing it as a missing but essential caching layer for scalable vector search.
\end{abstract}

\begin{comment}
    Vector databases have become a cornerstone of modern information retrieval, powering applications in recommendation, search, and retrieval-augmented generation (RAG) pipelines. While caching is a well-studied problem in traditional databases, a dedicated caching layer is absent in the vector search literature. Disk-based vector search suffers from high latencies in high-accuracy settings, whereas fully in-memory solutions cannot scale to billion-scale datasets. We present QVCache, the first backend-agnostic vector search cache designed to fill this gap, capable of working with any existing vector database. QVCache delivers sub-millisecond query latencies, achieving up to 300$\times$ lower latency on a cache hit compared to disk-based vector databases for billion-scale datasets. It uses only a megabyte-scale memory footprint by keeping in memory a very small hot set of vectors regardless of dataset size, which is negligible compared to fully in-memory systems for practical workloads. Unlike conventional caches, QVCache addresses the unique challenge of similarity caching, where cache hits cannot be determined by exact key matches. It solves this problem by applying an online learning algorithm that dynamically adjusts spatial distance thresholds governing cache hit decisions and keeps the cache size bounded through an eviction policy. QVCache is particularly effective for workloads exhibiting temporal-semantic locality, achieving orders of magnitude lower latencies on cache hits while maintaining recall comparable to that of the backend database. Our results establish QVCache as a missing but crucial caching layer for efficient large-scale similarity search.
\end{comment}