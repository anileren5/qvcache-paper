\section{Introduction}
%Needs to answer:
%1)necessity, whats the problem
%2)impact of the solution
%3)applicability to today's issues

% introduce the topic
Modern data-intensive services increasingly rely on Approximate Nearest Neighbor (ANN) search over high-dimensional embeddings. However, scaling ANN to high recall under strict latency SLOs remains fundamentally constrained by memory capacity and I/O bandwidth.
This tension is now a dominant systems bottleneck: improving recall predictably inflates latency and operational cost, while controlling latency requires either aggressive approximation or prohibitively large memory footprints. As a result, ANN has become a first-order performance and cost concern in production systems, including recommendation pipelines~\cite{Covington:2016:DNN:YouTube}, search engines~\cite{Liu:2021:Q2S:Facebook, amazon-shop-the-look, visrel}, and Retrieval-Augmented Generation (RAG) workloads for large language models (LLMs)~\cite{Lewis:2020:RAG:NeurIPS, gao2024retrievalaugmentedgenerationlargelanguage}. 

% the problem
This bottleneck is structural rather than incidental. Achieving high recall in ANN search requires traversing increasingly large and irregular neighborhoods in high-dimensional space, leading to random access amplification that cannot be efficiently prefetched or batched. In graph- and quantization-based indexes, recall improvements translate directly into expanded graph exploration, candidate explosion, and deeper disk or memory accesses. 
%Importantly, this tradeoff is not specific to a particular index structure: graph-based, quantization-based, and tree-based ANN indexes all incur increasing random access as recall increases. 
Although in-memory ANN systems achieve low latency, their memory cost grows linearly with the size of the dataset. Disk-based systems reduce memory pressure, but suffer severe latency degradation at high recall and large result sets $k$, where pointer chasing and random I/O dominate execution time. As a result, operators are forced into an undesirable choice between cost and performance, with no mechanism to amortize repeated query work over time.

% enabler
A natural response to repeated expensive computation is caching. Across the systems stack, caching has been the primary mechanism for amortizing work, from hardware memory hierarchies~\cite{memory-hierarchy} to database buffer pools~\cite{dbmin,data-caching-for-enterprise-scale}, intermediate results caching~\cite{caching-intermediate-results}, and final query result caches~\cite{memcached,redis2025}. These techniques rely on a core assumption: identical queries recur. This assumption underlies the cache abstraction itself, which treats queries as discrete keys rather than points in a metric space. However, in vector search, this assumption is invalid. Even minor changes in user inputs or prompts produce distinct embeddings, making exact-match caching ineffective~\cite{Covington:2016:DNN:YouTube, gao2024retrievalaugmentedgenerationlargelanguage}. Consequently, ANN systems effectively forfeit one of the most powerful tools in the systems toolbox.

% solutions so far
Existing ANN systems rely on index-internal heuristics—such as caching centroids~\cite{diskann} or upper graph layers~\cite{tiered-cache-hnsw}—to reduce average I/O. These mechanisms reduce constant factors but cannot eliminate backend execution, cannot exploit workload locality at the query level, and cannot generalize across ANN backends. Consequently, operators are forced into a persistent tradeoff between memory cost and latency, with no mechanism to amortize query work across time.

% insight
The key insight of this work is that query repetition in vector search is semantic rather than exact. Real workloads exhibit temporal locality in embedding space: while query vectors are rarely identical, they are often sufficiently close that their nearest-neighbor sets are interchangeable at target recall. Exploiting this locality requires treating caching as a similarity problem rather than an exact lookup problem~\cite{similarity-caching}. However, similarity caching in high-dimensional vector spaces introduces two unresolved systems challenges: (1) selecting similarity thresholds that preserve recall without collapsing hit rates, and (2) performing cache lookups efficiently without turning the cache itself into a nearest neighbor index with unbounded latency and memory growth. Naively addressing either challenge leads to unbounded false positives that lower recall, or to cache designs whose overhead rivals the backend ANN system.

Prior work on similarity caching provides theoretical foundations but does not address the constraints of ANN serving systems~\cite{similarity-caching,similarity-caching-theory-algorithms}. Existing practical systems for LLM response caching rely on fixed global thresholds~\cite{gptcache}, which perform poorly under heterogeneous query distributions, or tightly couple caching logic to a specific index structure~\cite{vcache}. None provide a general, query-level cache that is backend-agnostic, recall-aware, and resource-bounded.

% our solution
We introduce \textbf{QVCache}, a new caching system for ANN search: a similarity-aware, recall-preserving query cache with fixed resource budgets. 
%Unlike traditional caches that map discrete keys to results, QVCache maps regions of the embedding space to recall-valid nearest-neighbor sets.
QVCache assigns region-specific similarity thresholds in the embedding space and dynamically adapts them using an online learning algorithm driven by observed nearest-neighbor distance statistics. This design bounds lookup latency and memory usage independently of dataset size, while aggressively short-circuiting backend ANN queries without compromising recall.

% impact of the solution
QVCache is designed as a drop-in layer for existing ANN systems. It requires no changes to index construction, applies uniformly to in-memory and disk-based backends. Unlike ANN indexes, QVCache does not grow with dataset size, does not aim for asymptotic recall completeness, and never replaces backend search on cache misses. By converting semantic locality into amortized query execution, QVCache reduces backend load and latency in high-recall settings without increasing memory pressure. This allows meeting latency SLOs at high recall without provisioning fully in-memory ANN indexes.

% contributions
The contributions of this paper are:
\begin{itemize}
    \item A formulation of query-level vector caching as a similarity-aware, recall-constrained systems problem.

    \item The design of QVCache, a backend-agnostic ANN cache with adaptive, region-specific similarity thresholds and bounded latency and memory, which we show can reduce end-to-end latency by up to \(40\text{–}1000\times\) when integrated with existing ANN systems, while maintaining a memory footprint typically below 1\% of fully in-memory indexes.

    \item An online threshold-learning algorithm driven by observed nearest-neighbor distance statistics.

    \item A workload generation benchmarking framework for controlled temporal-semantic locality in vector search, enabling reproducible evaluation of vector caching systems.
\end{itemize}

\begin{comment}

Approximate Nearest Neighbor (ANN) search is a core primitive in modern data-intensive systems, underpinning applications such as recommendation systems~\cite{Covington:2016:DNN:YouTube}, search engines~\cite{Liu:2021:Q2S:Facebook, amazon-shop-the-look, visrel}, and large language models (LLMs) via Retrieval-Augmented Generation (RAG)~\cite{Lewis:2020:RAG:NeurIPS, gao2024retrievalaugmentedgenerationlargelanguage}. In these systems, user inputs, such as queries or prompts often augmented with contextual information~\cite{task-embeddings-learning-query-embeddings-using-task-context}, are mapped to high-dimensional embeddings using neural models~\cite{devlin2019bertpretrainingdeepbidirectional,mikolov2013efficientestimationwordrepresentations}. These embeddings are designed to preserve semantic similarity, placing related inputs close to one another in a shared vector space. Likewise, system objects, including videos~\cite{Covington:2016:DNN:YouTube}, products~\cite{Liu:2021:Q2S:Facebook}, and documents~\cite{distributed-representations-of-sentences-and-documents}, are represented as vectors in the same space. Answering a query therefore reduces to performing a vector search that retrieves the nearest object vectors to the query embedding. However, serving such vector search queries at scale presents a fundamental tension between memory cost and latency.


Nearest neighbor search is inherently computationally expensive, with a time and space complexity of $O(n \cdot d)$ where $n$ is the number of vectors and $d$ is their dimensionality~\cite{graph-based-vector-search-evaluation}. For this reason, exhaustive search is impractical in most scenarios, particularly for large-scale datasets. Consequently, nearly all vector databases employ various approximation techniques to reduce the number of comparisons~\cite{HNSWMalkovY16,diskann, product-quantization, spann,locality-sensitive-hashing,kd-tree-nn, faiss} and to lower the memory footprint~\cite{product-quantization}.

Due to the approximate nature of nearest neighbor search, there is a fundamental trade-off between search accuracy (recall) and performance (latency and throughput)~\cite{ann-analyses}. Among the various approaches, graph-based indexing methods ~\cite{diskann,HNSWMalkovY16,elpis, nsg} and inverted file indexes (IVF) ~\cite{product-quantization, faiss, spann}    have emerged as the most effective in balancing this trade-off. During index construction, graph-based methods build a proximity graph where each node represents a vector stored in the database, and edges connect vectors that are close to each other in the vector space. During query processing, the search begins from a pre-selected set of entry points and proceeds in a best-first manner until no closer candidate vectors are found ~\cite{graph-based-vector-search-evaluation, diskann, elpis, HNSWMalkovY16, nsg}.
On the other hand, IVF indexes partition the high-dimensional space into multiple regions. During query processing, the partitions closest to the query vector are identified, and similarity comparisons are performed only within those selected partitions.

While some systems store both the index and vectors on disk, primarily SSDs~\cite{diskann,spann,scalablediskbasedapproximatenearest}, others keep them entirely in memory~\cite{faiss}, and some support both options~\cite{milvus,weaviate2025,qdrant2025}, allowing users to select the storage medium for the vectors and index based on factors such as dataset size and performance requirements. In-memory systems have very high memory footprints, which increase substantially with dataset size, even during search, often reaching hundreds of gigabytes for billion-scale datasets~\cite{graph-based-vector-search-evaluation}. In contrast, disk-based systems can operate on commodity machines with tens of gigabytes of memory. However, they suffer from significantly higher latencies in high-recall settings; latency increases while throughput decreases exponentially as recall requirements grow due to excessive random accesses to disk ~\cite{diskann,scalablediskbasedapproximatenearest,parlayann, turbocharging-vector-databases-ssds}. This performance degradation becomes more severe when queries request a larger number of neighbors, as each query triggers exponentially more comparisons and random disk accesses ~\cite{parlayann}.

Over the decades, system researchers have leveraged caching to avoid repeating expensive operations, exploring it at various layers of modern systems, from the hardware stack~\cite{memory-hierarchy} to the application layer~\cite{caching-search-query-results}. This problem is particularly well-studied in databases, which cache pages at the storage layer~\cite{dbmin,data-caching-for-enterprise-scale}, intermediate results~\cite{caching-intermediate-results}, or final query results~\cite{memcached,redis2025}. Despite these efforts, caching in vector search remains largely unexplored~\cite{vdbms-survey-sigmod,vdbms-survey-extensive}. Existing approaches are limited and superficial: for instance, DiskANN~\cite{diskann} stores points near the centroid of the graph to reduce disk accesses during queries, and Tiered Cache-HNSW~\cite{tiered-cache-hnsw} keeps part of the highest layer of HNSW in memory. Both methods are system-level caches embedded within their respective systems, reducing disk access but not eliminating it entirely.

To the best of our knowledge, building a query-level cache for ANN search similar to Redis~\cite{redis2025} or Memcached~\cite{memcached}, which relies on the assumption that queries exhibit temporal locality, has not been attempted. This is a challenging problem in the context of vector search, as it is highly unlikely to receive the \emph{exact same} query vector multiple times. Even small changes in the user query or prompt produce different (but similar) vectors during 
the embedding generation step~\cite{Covington:2016:DNN:YouTube, gao2024retrievalaugmentedgenerationlargelanguage}.

This characteristic makes vector caching a form of the \textit{similarity caching} problem ~\cite{similarity-caching}. Unlike exact caching, similarity caching serves a query $q$ from the cache if it contains an entry for a sufficiently similar query $q'$, i.e., when $d(q, q') < r$, where $r$ is a similarity threshold defined in a metric space~\cite{similarity-caching,similarity-caching-theory-algorithms}. Another instance of similarity caching arises in LLM response caching. For example, GPTCache~\cite{gptcache} employs a static similarity threshold to determine cache hits, whereas vCache~\cite{vcache} highlights the importance of using adaptive thresholds for each cached prompt to optimize hit rates.

We present QVCache, the first query-level vector cache designed to address the unique challenges of similarity caching in vector search. QVCache assigns distinct similarity thresholds to different regions of the high-dimensional vector space and dynamically adjusts these thresholds using an online learning algorithm. It ensures bounded cache hit latency and a bounded memory footprint, independent of the dataset size, through a novel cache eviction strategy. Furthermore, QVCache leverages the distance between a query vector and its furthest nearest neighbor to learn these thresholds, enabling it to achieve a recall close to that of the underlying backend vector database.

In summary, the main contributions of this paper are as follows:

\begin{itemize}
    \item We provide a comprehensive analysis of the vector caching problem, highlighting challenges in selecting similarity thresholds and handling cache lookups that incur non-\(O(1)\) complexity.

    \item We introduce QVCache, the first query-level vector cache, which achieves up to \(300\times\) lower latency on cache hits compared to disk-based systems, while requiring only 0.1\% of the memory footprint of fully in-memory systems, and we open-source its implementation.

    \item  We propose a benchmarking framework for generating vector search workloads with configurable temporal–semantic locality characteristics, facilitating and encouraging further research on vector caching.

\end{itemize}

\end{comment}
