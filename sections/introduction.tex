\section{Introduction}
Approximate Nearest Neighbor (ANN) search is a core primitive in modern data-intensive systems, underpinning applications such as recommendation systems~\cite{Covington:2016:DNN:YouTube}, search engines~\cite{Liu:2021:Q2S:Facebook, amazon-shop-the-look, visrel}, and large language models (LLMs) via Retrieval-Augmented Generation (RAG)~\cite{Lewis:2020:RAG:NeurIPS, gao2024retrievalaugmentedgenerationlargelanguage}. In these systems, user inputs, such as queries or prompts often augmented with contextual information~\cite{task-embeddings-learning-query-embeddings-using-task-context}, are mapped to high-dimensional embeddings using neural models~\cite{devlin2019bertpretrainingdeepbidirectional,mikolov2013efficientestimationwordrepresentations}. These embeddings are designed to preserve semantic similarity, placing related inputs close to one another in a shared vector space. Likewise, system objects, including videos~\cite{Covington:2016:DNN:YouTube}, products~\cite{Liu:2021:Q2S:Facebook}, and documents~\cite{distributed-representations-of-sentences-and-documents}, are represented as vectors in the same space. Answering a query therefore reduces to performing a vector search that retrieves the nearest object vectors to the query embedding. However, serving such vector search queries at scale presents a fundamental tension between memory cost and latency.


Nearest neighbor search is inherently computationally expensive, with a time and space complexity of $O(n \cdot d)$ where $n$ is the number of vectors and $d$ is their dimensionality~\cite{graph-based-vector-search-evaluation}. For this reason, exhaustive search is impractical in most scenarios, particularly for large-scale datasets. Consequently, nearly all vector databases employ various approximation techniques to reduce the number of comparisons~\cite{HNSWMalkovY16,diskann, product-quantization, spann,locality-sensitive-hashing,kd-tree-nn, faiss} and to lower the memory footprint~\cite{product-quantization}.

Due to the approximate nature of nearest neighbor search, there is a fundamental trade-off between search accuracy (recall) and performance (latency and throughput)~\cite{ann-analyses}. Among the various approaches, graph-based indexing methods ~\cite{diskann,HNSWMalkovY16,elpis, nsg} and inverted file indexes (IVF) ~\cite{product-quantization, faiss, spann}    have emerged as the most effective in balancing this trade-off. During index construction, graph-based methods build a proximity graph where each node represents a vector stored in the database, and edges connect vectors that are close to each other in the vector space. During query processing, the search begins from a pre-selected set of entry points and proceeds in a best-first manner until no closer candidate vectors are found ~\cite{graph-based-vector-search-evaluation, diskann, elpis, HNSWMalkovY16, nsg}.
On the other hand, IVF indexes partition the high-dimensional space into multiple regions. During query processing, the partitions closest to the query vector are identified, and similarity comparisons are performed only within those selected partitions.

While some systems store both the index and vectors on disk, primarily SSDs~\cite{diskann,spann,scalablediskbasedapproximatenearest}, others keep them entirely in memory~\cite{faiss}, and some support both options~\cite{milvus,weaviate2025,qdrant2025}, allowing users to select the storage medium for the vectors and index based on factors such as dataset size and performance requirements. In-memory systems have very high memory footprints, which increase substantially with dataset size, even during search, often reaching hundreds of gigabytes for billion-scale datasets~\cite{graph-based-vector-search-evaluation}. In contrast, disk-based systems can operate on commodity machines with tens of gigabytes of memory. However, they suffer from significantly higher latencies in high-recall settings; latency increases while throughput decreases exponentially as recall requirements grow due to excessive random accesses to disk ~\cite{diskann,scalablediskbasedapproximatenearest,parlayann, turbocharging-vector-databases-ssds}. This performance degradation becomes more severe when queries request a larger number of neighbors, as each query triggers exponentially more comparisons and random disk accesses ~\cite{parlayann}.

Over the decades, system researchers have leveraged caching to avoid repeating expensive operations, exploring it at various layers of modern systems, from the hardware stack~\cite{memory-hierarchy} to the application layer~\cite{caching-search-query-results}. This problem is particularly well-studied in databases, which cache pages at the storage layer~\cite{dbmin,data-caching-for-enterprise-scale}, intermediate results~\cite{caching-intermediate-results}, or final query results~\cite{memcached,redis2025}. Despite these efforts, caching in vector search remains largely unexplored~\cite{vdbms-survey-sigmod,vdbms-survey-extensive}. Existing approaches are limited and superficial: for instance, DiskANN~\cite{diskann} stores points near the centroid of the graph to reduce disk accesses during queries, and Tiered Cache-HNSW~\cite{tiered-cache-hnsw} keeps part of the highest layer of HNSW in memory. Both methods are system-level caches embedded within their respective systems, reducing disk access but not eliminating it entirely.

To the best of our knowledge, building a query-level cache for ANN search similar to Redis~\cite{redis2025} or Memcached~\cite{memcached}, which relies on the assumption that queries exhibit temporal locality, has not been attempted. This is a challenging problem in the context of vector search, as it is highly unlikely to receive the \emph{exact same} query vector multiple times. Even small changes in the user query or prompt produce different (but similar) vectors during 
the embedding generation step~\cite{Covington:2016:DNN:YouTube, gao2024retrievalaugmentedgenerationlargelanguage}.

This characteristic makes vector caching a form of the \textit{similarity caching} problem ~\cite{similarity-caching}. Unlike exact caching, similarity caching serves a query $q$ from the cache if it contains an entry for a sufficiently similar query $q'$, i.e., when $d(q, q') < r$, where $r$ is a similarity threshold defined in a metric space~\cite{similarity-caching,similarity-caching-theory-algorithms}. Another instance of similarity caching arises in LLM response caching. For example, GPTCache~\cite{gptcache} employs a static similarity threshold to determine cache hits, whereas vCache~\cite{vcache} highlights the importance of using adaptive thresholds for each cached prompt to optimize hit rates.

We present QVCache, the first query-level vector cache designed to address the unique challenges of similarity caching in vector search. QVCache assigns distinct similarity thresholds to different regions of the high-dimensional vector space and dynamically adjusts these thresholds using an online learning algorithm. It ensures bounded cache hit latency and a bounded memory footprint, independent of the dataset size, through a novel cache eviction strategy. Furthermore, QVCache leverages the distance between a query vector and its furthest nearest neighbor to learn these thresholds, enabling it to achieve a recall close to that of the underlying backend vector database.

In summary, the main contributions of this paper are as follows:

\begin{itemize}
    \item We provide a comprehensive analysis of the vector caching problem, highlighting challenges in selecting similarity thresholds and handling cache lookups that incur non-\(O(1)\) complexity.

    \item We introduce QVCache, the first query-level vector cache, which achieves up to \(300\times\) lower latency on cache hits compared to disk-based systems, while requiring only 0.1\% of the memory footprint of fully in-memory systems, and we open-source its implementation.

    \item  We propose a benchmarking framework for generating vector search workloads with configurable temporalâ€“semantic locality characteristics, facilitating and encouraging further research on vector caching.

\end{itemize}

