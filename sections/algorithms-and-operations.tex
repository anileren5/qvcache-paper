\section{Algorithms and Operations}
This section presents the overall architecture of QVCache, the unique challenges it addresses, and the algorithms and operations underlying its core components.

\subsection{Tiered Search}

\begin{algorithm}[htbp]
\caption{\textsc{TieredSearch}}
\label{alg:tiered-search}
\begin{algorithmic}[1]
\State \textbf{Input:} Query vector $Q$, result size $k$
\State \textbf{Output:} IDs of the $k$ nearest neighbors, their distances to $Q$
\State $(\text{ID}_{\text{cache}}, \text{d}_{\text{cache}}, \text{isHit}) \gets \textsc{CacheSearch}(Q, k)$ \label{alg:line:cachesearch}
\If{$\text{isHit}$} \label{alg:line:ishit}
    \State \Return $(\text{ID}_{\text{cache}}, \text{d}_{\text{cache}})$ \label{alg:line:returnhit}
\Else
    \State $(\text{ID}_{\text{backend}}, \text{d}_{\text{backend}}) \gets \textsc{BackendSearch}(Q, k)$ \label{alg:line:backendsearch}
    \State \textsc{AsyncCacheFill}$(\text{ID}_{\text{backend}})$ \label{alg:line:async-cache-fill}
    \State \textsc{AsyncLearnThreshold}$(Q, \text{d}_{\text{backend}}, k)$ \label{alg:line:async-learn-threshold}
    \State \Return $(\text{ID}_{\text{backend}}, \text{d}_{\text{backend}})$ \label{alg:line:returnmiss}
\EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
    \caption{\textsc{CacheSearch}}
    \label{alg:cache-search}
    \begin{algorithmic}[1]
    \State \textbf{Input:} Query vector $Q$, result size $k$, search strategy $\sigma \in \{\text{EAGER}, \text{EXHAUSTIVE}, \text{ADAPTIVE}\}$
    \State \textbf{Output:} IDs of candidate neighbors $\text{ID}_{\text{cache}}$, their distances $\text{d}_{\text{cache}}$ to $Q$, hit flag $\text{isHit}$
    \If{$\sigma = \text{ADAPTIVE}$} \label{alg:line:adaptive-check}
        \State $\text{hitRatio} \gets \textsc{GetHitRatioTrend}()$
        \State $\sigma \gets \begin{cases} \text{EXHAUSTIVE} & \text{if } \text{hitRatio} < \text{thresholdHitRatio} \\ \text{EAGER} & \text{otherwise} \end{cases}$ \label{alg:line:adaptive-switch}
        \State \Return $\textsc{CacheSearch}(Q, k, \sigma)$ \label{alg:line:return-adaptive}
    \Else \label{alg:line:eager-sequential}
        \State $R \gets \textsc{Reverse}(\textsc{GetEvictionOrder}())$ \label{alg:line:get-eviction-order}
        \State $(\text{ID}_c, \text{d}_c, \text{isHit}) \gets ([], [], \text{False})$ \Comment{candidates} \label{alg:line:init-candidates}
        \For{each mini-index $i \in R$} \label{alg:line:for-mini-indices}
            \State $(\text{ID}_i, \text{d}_i) \gets \textsc{SearchMiniIndex}(i, Q, k)$ \label{alg:line:search-mini-index}
            \If{$\textsc{IsHit}(Q, k, \text{d}_i)$} \label{alg:line:check-hit}
                \State $\text{ID}_c \gets \text{ID}_c.\text{concat}(\text{ID}_i)$, $\text{d}_c \gets \text{d}_c.\text{concat}(\text{d}_i)$ \label{alg:line:add-candidates}
                \State $\text{isHit} \gets \text{True}$ \label{alg:line:set-hit}
                \State $\textsc{UpdateMiniIndexEvictionMetadata}(i)$ \label{alg:line:update-lru}
                \If{$\sigma = \text{EAGER}$} \label{alg:line:check-eager}
                    \State \textbf{break} \label{alg:line:break-eager}
                \EndIf
            \EndIf
        \EndFor
        \State $(\text{ID}_c, \text{d}_c) \gets \textsc{ReRank}(\text{ID}_c, \text{d}_c)$ \label{alg:line:rerank}
        \State \Return $(\text{ID}_c[:k], \text{d}_c[:k], \text{isHit})$ \label{alg:line:return-results}
    \EndIf
    \end{algorithmic}
\end{algorithm}





QVCache is a query-level vector cache, analogous to systems like Redis~\cite{redis2025}, that operates in front of the main vector database (referred to as the backend database). Upon receiving a query, QVCache first attempts to answer it directly from the cache (Line \ref{alg:line:cachesearch} in Algorithm \ref{alg:tiered-search}). If it determines that the cached result is sufficiently reliable (Line \ref{alg:line:ishit}), the response is served without accessing the backend (Line \ref{alg:line:returnhit}). Otherwise, the query is forwarded to the backend database for processing (Lines \ref{alg:line:backendsearch}, \ref{alg:line:returnmiss}). In the background, asynchronous tasks fetch vectors from the backend and insert them into QVCache’s in-memory index structures, i.e., mini-indexes (Line \ref{alg:line:async-cache-fill}) and update the distance thresholds (Line \ref{alg:line:async-learn-threshold}) used in Algorithm \ref{alg:is-hit}. QVCache is \textit{backend-agnostic}, working with any vector database that provides the following interfaces, which most systems support:
\begin{itemize}
    \item \texttt{search(Q, k) → (ID[], d[])}: retrieves the nearest top-$k$ neighbor IDs and their distances to a query vector $Q$.
    \item \texttt{fetch(ID[]) → Vector[]}: fetches the vectors corresponding to the given IDs.
\end{itemize}

\begin{algorithm}[htbp]
\caption{\textsc{IsHit}}
\label{alg:is-hit}
\begin{algorithmic}[1]
\State \textbf{Input:} Query vector $Q$, result size $k$, distances of candidate neighbors to query $\text{d}_{\text{cache}}$ (sorted in ascending order)
\State \textbf{Output:} True for cache hit, False otherwise
\State $R \gets \textsc{ComputeRegionKey}(Q)$ \label{alg:line:compute-region-key} \
\If{$\text{d}_{\text{cache}}[k] \le (1 + D) \cdot \theta[k][R]$} \label{alg:line:cachecheck}
    \State \Return True
\EndIf
\State \Return False
\end{algorithmic}
\end{algorithm}

\subsection{Mini-indexes}
\label{sec:mini-indexes} QVCache maintains its in-memory vectors in multiple mini-indexes, each an instance of a FreshVamana graph~\cite{freshdiskann}, and manages them concurrently. This design narrows the search space during lookups (\text{EAGER} strategy in Algorithm~\ref{alg:cache-search}) and allows each mini-index to be treated as an independent unit for cache eviction.

\textbf{Cache Fill.}
QVCache maintains eviction metadata to track access patterns (i.e. cache hits, evictions, and fills) across its mini-indexes, ranking them from hottest to coldest based on the chosen eviction policy. Under the default LRU policy, the hottest mini-index corresponds to the most recently used one. Upon a cache miss, the system attempts to insert the new vector(s) into a mini-index (Line \ref{alg:line:async-cache-fill} in Algorithm \ref{alg:tiered-search}) in order of decreasing temperature, starting from the hottest and proceeding to progressively colder ones until it finds a mini-index with sufficient free capacity to accommodate all $k$ vectors, ensuring that all vectors from a single cache miss are colocated within the same mini-index rather than fragmented across multiple ones. If no mini-index has free capacity, QVCache evicts the coldest one according to the eviction policy, inserts the vector into that mini-index, and promotes it to be the hottest (similar to line \ref{alg:line:update-lru} in Algorithm \ref{alg:cache-search}). For example, in Figure~\ref{fig:architecture}, the system inserts into Mini-index~2, identified as the hottest according to the eviction policy.

\textbf{Cache Search.} In contrast to traditional caches with constant-time lookup, scanning a mini-index in QVCache (\textsc{SearchMiniIndex} in Algorithm~\ref{alg:cache-search}) incurs a time complexity of $O(\log c_{\text{mini-index}})$, where $c_{\text{mini-index}}$ denotes the capacity of each mini-index. Because QVCache maintains multiple mini-indexes, the worst-case lookup cost grows with both their number and size. Formally, let $C(N)$ denote the cost of searching a cache with capacity $N$ vectors,
partitioned into $n_{\text{mini-index}}$ mini-indexes, each storing $c_{\text{mini-index}}$ vectors.
The worst-case lookup cost then grows as

\begin{equation}
C(N) \propto n_{\text{mini-index}} \cdot \log\left(c_{\text{mini-index}}\right).
\label{exp:cache-search-cost}
\end{equation}

%Because nearest neighbors are colocated within a single mini-index during Cache Fill, searching only a small subset is sufficient.
To reduce this cost, QVCache tries to minimize the number of mini-indexes it scans. 
As in Cache Fill, it scans mini-indexes in descending heat order (Line~\ref{alg:line:get-eviction-order} in Algorithm~\ref{alg:cache-search}). Upon detecting a hit, QVCache inserts the corresponding neighbors into the candidate set (Lines~\ref{alg:line:check-hit}, \ref{alg:line:add-candidates}). Under the \texttt{EAGER} strategy, the search terminates after the first hit, whereas \texttt{EXHAUSTIVE} scans all mini-indexes and re-ranks the union of retrieved candidates (Line~\ref{alg:line:rerank}). Mini-indexes that contribute candidates to a cache hit are promoted to the hottest set (Line~\ref{alg:line:update-lru}). Because \texttt{EAGER} scans mini-indexes in the same heat order used during Cache Fill, and neighbors from the same cache miss are colocated in these hottest mini-indexes, it typically needs to scan only the first few mini-indexes to detect a cache hit (when one exists), effectively eliminating the linear factor in the cost expression in~\ref{exp:cache-search-cost}.

While \texttt{EXHAUSTIVE} strategy is more effective at recovering high-quality candidate sets, its cost grows linearly with $n{_\text{mini-index}}$. In contrast, \texttt{EAGER} exhibits near constant-time behavior with respect to $n{_\text{mini-index}}$ in typical settings; under high hit rates, the first mini-index alone often suffices. To balance these trade-offs, QVCache’s \texttt{ADAPTIVE} policy monitors recent hit ratios and switches between \texttt{EAGER} and \texttt{EXHAUSTIVE}: when the hit rate exceeds a threshold (e.g.,~0.9), it uses \texttt{EAGER}; otherwise, it falls back to \texttt{EXHAUSTIVE}. Empirically, \texttt{ADAPTIVE} performs well, though users may choose any strategy depending on their recall–latency requirements.

\vspace{1ex}

\textbf{Cache Eviction.}
\label{sec:eviction}
FreshVamana handles deletions in two stages. When a node is marked for deletion, it is first flagged as inactive and excluded from subsequent searches, but its memory remains allocated. Periodically, a background \textit{consolidation} process reclaims memory by removing all flagged nodes and reconnecting the surrounding graph. We observed that this lazy deletion strategy introduces two major drawbacks: (1) the consolidation step requires multi-threaded execution, adding significant overhead ~\cite{turbocharging-vector-databases-ssds}  and reducing throughput for workloads with frequently changing working sets ~\cite{working-set}, and (2) infrequent consolidation can cause memory bloat due to accumulated deleted nodes, which is problematic in memory-constrained environments where QVCache operates.

Because of these limitations, QVCache adopts mini-indexes as the unit of eviction. This enables to avoid costly consolidation operation in FreshVamana \cite{freshdiskann}. When all the mini-indexes get full, it evicts one of them according to eviction policy, e.g. Mini-index 3 in Figure~\ref{fig:architecture}, and promoted to the hottest index for the future insertions to be used. 

The larger the mini-indexes, the greater the information loss upon eviction: evicting a single mini-index removes a larger set of cached vectors at once. In the extreme case where $n_{\text{mini-index}} = 1$, a single eviction discards the entire cache. This motivates partitioning the cache into finer-grained units via multiple mini-indexes, thereby reducing eviction-induced information loss. However, increasing $n_{\text{mini-index}}$ also increases lookup latency through the linear factor in $n_{\text{mini-index}}$ in Equation~\ref{exp:cache-search-cost}. We study this capacity–partitioning trade-off empirically in Section~\ref{sec:granularity-matters}.

\vspace*{-\baselineskip} 

\subsection{Cache Hit and Miss Decisions}
\label{sec:threshold-challenges}
Deciding if a vector search query can be answered from cache (Algorithm~\ref{alg:is-hit}) is a similarity caching problem~\cite{similarity-caching, similarity-caching-theory-algorithms}. A query is considered a cache hit if the distance of the query vector to the furthest vector, $d_{\text{cache}}[k]$, in the candidate neighbor set is less than or equal to a \textit{similarity threshold}, $\theta[k][R]$, where $R$ is the identifier of the region the query falls into.. Determining the optimal threshold to maximize hit ratios while maintaining the recall of the backend database is challenging. If the threshold is too small, cache misses occur too frequently. If it is too large, cache hits return very different results from the backend, resulting in low recalls. We list four key challenges around the cache hit and miss decisions and respective solutions we propose.

\vspace{1ex}
 \textbf{Challenge 1: No universal threshold across datasets.} Different datasets may require substantially different distance thresholds, making manual tuning impractical.  

 \textit{Solution: Learned thresholds.} QVCache infers distance thresholds based on cache misses (Algorithm~\ref{alg:learn-threshold}), thereby automatically adapting these thresholds across different datasets.
 

\vspace{1ex}
 \textbf{Challenge 2: No universal threshold across data regions.} Vectors in a database are distributed non-uniformly across the high-dimensional space. Some regions are dense and highly clustered, while others are sparse. %, and certain regions contain more vectors than others.
 Consequently, a single global similarity threshold may perform well for some queries but poorly for others, even though the dataset is the same.

 \textit{Solution: Spatial thresholds.} It partitions the high-dimensional space into sub-regions (sub-spaces) and assigns a separate similarity threshold to each. These thresholds are learned independently, and for a given query, QVCache uses the threshold corresponding to the sub-region (sub-space) onto which the query vector falls to make cache hit decisions.

 %\begin{figure}[h]
 % \centering
 % \begin{minipage}[t]{0.48\linewidth}
 %   \centering
 %   \includegraphics[width=\linewidth]{figures/first.pdf}% first image
 % \end{minipage}
 % \hfill
 % \begin{minipage}[t]{0.48\linewidth}
 %   \centering
 %   \includegraphics[width=\linewidth]{figures/second.pdf}% second image
 % \end{minipage}
 % \caption{Initial queries help QVCache learn spatial similarity thresholds (left), and subsequent queries falling into these regions are classified as cache hits or misses (right). Blue dots represent data vectors, green dots queries resulting in cache hits, and red dots queries resulting in cache misses.}
 % \label{fig:spatial-thresholds}
%\end{figure}

\vspace{1ex}
    \textbf{Challenge 3: No universal thresholds over time.} Even within the same sub-region, query distribution may change, so the threshold that yields the best cache hit/miss decisions may change over time.
    
    \textit{Solution: Continuous learning.} QVCache runs Algorithm~\ref{alg:learn-threshold} continuously to adapt its thresholds, allowing it to track changes in query patterns.

\vspace{1ex}
 \textbf{Challenge 4: No universal threshold across different $k$ values.} As the parameter $k$ in a vector search query increases, the system generally requires a higher threshold. However, the threshold for a larger $k$ (e.g., $k=10$) cannot be inferred from that of a smaller $k$ (e.g., $k=1$), as the relationship is highly nonlinear and often unpredictable across datasets \cite{10.14778/3712221.3712224}.

 \textit{Solution: $k$-dependent thresholds.} QVCache maintains an independent threshold for each observed $k$ and learns them separately.

\vspace{1ex}
To implement these solutions, QVCache maintains a 2D array of thresholds, $\theta$. As shown in Algorithms ~\ref{alg:is-hit} and ~\ref{alg:learn-threshold} , the first dimension is resolved by the parameter $k$, while the second corresponds to the region $R$ into which the query falls, which is computed by \textsc{ComputeRegionKey} and described in detail in Section~\ref{sec:method:spatial-thresholding}. When evaluating whether a query is a cache hit, the system does not directly compare the distance of the furthest vector in the candidate set, $\text{d}_{\text{cache}}[k]$, with the threshold. Instead, it applies a multiplicative adjustment using the \textit{deviation factor}, $D$ (Line \ref{alg:line:cachecheck}). This factor serves as a tunable knob that balances cache hit ratio and recall: increasing $D$ raises the hit ratio but may lead to reduced recall. Such a knob provides flexibility for users to adapt QVCache to systems with different accuracy and performance requirements beyond the learned thresholds. In our experiments, we found that setting $D$ in the range $[0, 0.5]$ provides a practical operating regime.


\begin{algorithm}[htbp]
\caption{\textsc{LearnThreshold}}
\label{alg:learn-threshold}
\begin{algorithmic}[1]
\State \textbf{Input:} Query vector $Q$, result size $k$, distances of neighbors returned by the backend $\text{d}_{\text{backend}}$ (sorted in ascending order)
\State $R \gets \textsc{ComputeRegionKey}(Q)$
\State $\theta[k][R] \gets (1 - \alpha) \cdot \theta[k][R] + \alpha \cdot \text{d}_{\text{backend}}[k]$ \label{alg:line:updatetheta}
\end{algorithmic}
\end{algorithm}

  \vspace*{-\baselineskip} 
\subsection{Learning Thresholds}
In  Algorithm~\ref{alg:is-hit}, QVCache uses the distance of the furthest vector in the candidate set generated by itself, $\text{d}_{\text{cache}}[k]$, to determine the cache hit/miss. On the other hand, Algorithm~\ref{alg:learn-threshold} (called from Line \ref{alg:line:async-learn-threshold} of Algorithm \ref{alg:tiered-search}) uses the distance of the furthest vector returned by the backend, $\text{d}_{\text{backend}}[k]$ to learn the spatial thresholds in case of cache misses,  $\theta[k][R]$ in  Algorithm~\ref{alg:is-hit}. Ideally, $\text{d}_{\text{cache}}[k]$ converges to $\text{d}_{\text{backend}}[k]$ for a cache hit. 

The threshold update mechanism in QVCache was inspired by the Adam optimizer~\cite{adamoptimizer}, which updates momentum using gradients during neural network training. Similarly, QVCache updates thresholds using feedback from backend query results. The update employs an \textit{adaptivity rate}, $\alpha$, which determines how quickly QVCache adapts to changes in the query distribution (Line \ref{alg:line:updatetheta} in Algorithm \ref{alg:learn-threshold}). 

The first term in the update equation preserves the momentum of past query behavior, while the second term enables adaptation to query distribution shifts. A higher adaptivity rate, $\alpha$, allows QVCache to adjust more quickly to such shifts but also causes it to forget past query patterns faster. This balance enables QVCache to remain both stable and responsive under dynamic workloads.
In our experiments, we found that setting $\alpha = 0.9$ provides a good trade-off between stability and responsiveness.

%For each query resulting in a cache miss, QVCache populates the cache by fetching the top-$k$ vectors from the backend by their IDs and inserting them into the appropriate mini-indexes (Section \ref{sec:mini-indexes}), while also updating the thresholds using Algorithm~\ref{alg:learn-threshold}. To avoid using stale index data that could degrade recall, threshold updates are performed only after corresponding top-$k$ vectors are inserted into mini-indexes. Fetching vectors from the backend can be costly depending on the deployment strategy (i.e. whether QVCache and the backend are co-located or the backend is remote) and the backend's access path (i.e. whether it reads vectors from disk or keeps them in memory) to vectors. QVCache avoids blocking the critical query path, minimizing latency by executing both the cache and threshold updates asynchronously through a pool of background threads, as described in Algorithm~\ref{alg:tiered-search}.

%Due to the asynchronous execution of these tasks, if QVCache receives a query before the completion of cache update triggered by previous similar queries, it may result in a cache miss. This occurs because the thresholds used during this period are still stale, even though the query would have been a cache hit after the update is applied.

\vspace*{-\baselineskip} 
\subsection{Scalable Spatial Thresholding in High Dimensions}\label{sec:method:spatial-thresholding}
QVCache partitions the high-dimensional vector space into sub-spaces by dividing each dimension of the vector space into ${\text{n}}_{\text{buckets}}$ buckets. Upon receiving a query, it identifies the bucket corresponding to each dimension (\textsc{ComputeRegionKey}) to determine the appropriate spatial threshold.

However, pre-allocating the buckets for all possible sub-spaces is infeasible, especially for high-dimensional vectors.
For example, SIFT dataset vectors \cite{sift} have 128 dimensions, where storing the thresholds for all possible sub-spaces requires $8^{128}$ thresholds for ${\text{n}}_{\text{buckets}}$=8. 

\textbf{Dimensionality reduction.} Queries that are close in the original high-dimensional space remain close after projection into a lower-dimensional space, so the proximity relationships relevant for cache-hit decisions are approximately preserved. Therefore, QVCache projects incoming query vectors onto a lower-dimensional space via Principal Component Analysis (PCA) \cite{principalcomponentanalysis} and uses this reduced space for both threshold assignment and learning. QVCache performs a lightweight offline training step to compute the PCA transformation matrix, and this process does not require access to the full dataset; in our experiments, random sampling of as little as 0.1\%–0.01\% of the dataset was sufficient. On the SIFT dataset~\cite{sift}, this PCA training took around 10 minutes with a 0.1\% sampling ratio on the machine described in Section~\ref{sec:experimental-setup}, whereas building the corresponding ANN index took roughly a week. Thus, the additional training cost for QVCache is negligible compared to index construction.

On top of the projection, QVCache applies a straightforward partitioning scheme, dividing each dimension of the reduced space into equal-sized buckets. More advanced partitioning strategies that account for dataset-specific characteristics could be explored as future work. The \textsc{ComputeRegionKey} function applies the learned PCA transformation to project each query into the reduced space and identifies its corresponding region by locating the buckets it falls into.



For instance, if ${\text{d}}_{\text{reduced}} = 16$, the memory requirement becomes $8^{16}$ floating points for ${\text{n}}_{\text{buckets}}$=8. Although this technique significantly reduces the memory footprint, it still exceeds the practical constraints under which QVCache is designed to operate.

\textbf{Lazy initialization.} Similar to how data vectors tend to cluster, queries also exhibit spatial locality, meaning that not every region of the vector space will receive queries. Consequently, QVCache does not allocate memory for thresholds in regions that have not yet been queried; a missing key implicitly represents a region which has not been queried yet. Memory is allocated only when the first query for a new region arrives and the threshold is initialized to $d_{backend}[k]$ (i.e. updating the threshold by setting $\alpha$ to 1 in Algorithm \ref{alg:learn-threshold}). Our experiments show that, at most 50,000 regions (using the SpaceV  dataset \cite{SPACEV1B_SPTAG} with ${\text{d}}_{\text{reduced}} = 16$ and ${\text{n}}_{\text{buckets}}$=8), out of $8^{16}$ possible regions, become active, consuming approximately 200KB of memory. If the number of active regions exceeds a user-defined limit, QVCache can evict thresholds using an eviction policy such as LRU, and re-learn them later as needed. This lazy initialization, combined with dimensionality reduction, allows QVCache to maintain an exceptionally low memory footprint.

\begin{figure}[b]
    \vspace{-\baselineskip}
    \centering
    \includegraphics[width=0.75\linewidth]{plots/noise_ratio_analysis/neighbor_overlap_analysis.pdf}
    \caption{\textbf{Nearest Neighbor overlap under perturbation (DEEP \cite{deep} dataset, $k=10$).} The overlap between the neighbor sets of the base and perturbed queries decays sharply, approaching near-zero at a noise ratio of 0.5.}
    \label{fig:overlap-analysis}
\end{figure}
