\section{Algorithms and Operations}
This section presents the overall architecture of QVCache, the unique challenges it addresses, and the algorithms and operations underlying its core components.

\subsection{Tiered Search}

\begin{algorithm}[htbp]
\caption{\textsc{TieredSearch}}
\label{alg:tiered-search}
\begin{algorithmic}[1]
\State \textbf{Input:} Query vector $Q$, result size $k$
\State \textbf{Output:} IDs of the $k$ nearest neighbors, their distances to $Q$
\State $(\text{ID}_{\text{cache}}, \text{d}_{\text{cache}}, \text{isHit}) \gets \textsc{CacheSearch}(Q, k)$ \label{alg:line:cachesearch}
\If{$\text{isHit}$} \label{alg:line:ishit}
    \State \Return $(\text{ID}_{\text{cache}}, \text{d}_{\text{cache}})$ \label{alg:line:returnhit}
\Else
    \State $(\text{ID}_{\text{backend}}, \text{d}_{\text{backend}}) \gets \textsc{BackendSearch}(Q, k)$ \label{alg:line:backendsearch}
    \State \textsc{AsyncCacheFill}$(\text{ID}_{\text{backend}})$ \label{alg:line:async-cache-fill}
    \State \textsc{AsyncLearnThreshold}$(Q, \text{d}_{\text{backend}}, k)$ \label{alg:line:async-learn-threshold}
    \State \Return $(\text{ID}_{\text{backend}}, \text{d}_{\text{backend}})$ \label{alg:line:returnmiss}
\EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
    \caption{\textsc{CacheSearch}}
    \label{alg:cache-search}
    \begin{algorithmic}[1]
    \State \textbf{Input:} Query vector $Q$, result size $k$, search strategy $\sigma \in \{\text{EAGER}, \text{SEQUENTIAL}, \text{ADAPTIVE}\}$
    \State \textbf{Output:} IDs of candidate neighbors $\text{ID}_{\text{cache}}$, their distances $\text{d}_{\text{cache}}$ to $Q$, hit flag $\text{isHit}$
    \If{$\sigma = \text{ADAPTIVE}$} \label{alg:line:adaptive-check}
        \State $\text{hitRatio} \gets \textsc{GetHitRatioTrend}()$
        \State $\sigma \gets \begin{cases} \text{SEQUENTIAL} & \text{if } \text{hitRatio} < \text{thresholdHitRatio} \\ \text{EAGER} & \text{otherwise} \end{cases}$ \label{alg:line:adaptive-switch}
        \State \Return $\textsc{CacheSearch}(Q, k, \sigma)$ \label{alg:line:return-adaptive}
    \Else \label{alg:line:eager-sequential}
        \State $R \gets \textsc{Reverse}(\textsc{GetEvictionOrder}())$ \label{alg:line:get-eviction-order}
        \State $(\text{ID}_c, \text{d}_c, \text{isHit}) \gets ([], [], \text{False})$ \Comment{candidates} \label{alg:line:init-candidates}
        \For{each mini-index $i \in R$} \label{alg:line:for-mini-indices}
            \State $(\text{ID}_i, \text{d}_i) \gets \textsc{SearchMiniIndex}(i, Q, k)$ \label{alg:line:search-mini-index}
            \If{$\textsc{IsHit}(Q, k, \text{d}_i)$} \label{alg:line:check-hit}
                \State $\text{ID}_c \gets \text{ID}_c.\text{concat}(\text{ID}_i)$, $\text{d}_c \gets \text{d}_c.\text{concat}(\text{d}_i)$ \label{alg:line:add-candidates}
                \State $\text{isHit} \gets \text{True}$ \label{alg:line:set-hit}
                \State $\textsc{UpdateMiniIndexEvictionMetadata}(i)$ \label{alg:line:update-lru}
                \If{$\sigma = \text{EAGER}$} \label{alg:line:check-eager}
                    \State \textbf{break} \label{alg:line:break-eager}
                \EndIf
            \EndIf
        \EndFor
        \State $(\text{ID}_c, \text{d}_c) \gets \textsc{ReRank}(\text{ID}_c, \text{d}_c)$ \label{alg:line:rerank}
        \State \Return $(\text{ID}_c[:k], \text{d}_c[:k], \text{isHit})$ \label{alg:line:return-results}
    \EndIf
    \end{algorithmic}
\end{algorithm}





QVCache is a query-level vector cache, analogous to systems like Redis~\cite{redis2025}, that operates in front of the main vector database (referred to as the backend database). Upon receiving a query, QVCache first attempts to answer it directly from the cache (Line \ref{alg:line:cachesearch} in Algorithm \ref{alg:tiered-search}). If it determines that the cached result is sufficiently reliable (Line \ref{alg:line:ishit}), the response is served without accessing the backend (Line \ref{alg:line:returnhit}). Otherwise, the query is forwarded to the backend database for processing (Lines \ref{alg:line:backendsearch}, \ref{alg:line:returnmiss}). In the background, asynchronous tasks fetch vectors from the backend and insert them into QVCache’s in-memory index structures i.e. mini-indexes (Line \ref{alg:line:async-cache-fill}) and update the distance thresholds (Line \ref{alg:line:async-learn-threshold}) used in Algorithm \ref{alg:is-hit}. QVCache is \textit{backend-agnostic}, working with any vector database that provides the following interfaces, which most modern systems support:
\begin{itemize}
    \item \texttt{search(Q, k) → (ID, d)}: retrieves the nearest top-$k$ neighbor IDs and their distances to a query vector $Q$.
    \item \texttt{fetch(ID[]) → Vector[]}: fetches the vectors corresponding to the given IDs.
\end{itemize}

\begin{algorithm}[htbp]
\caption{\textsc{IsHit}}
\label{alg:is-hit}
\begin{algorithmic}[1]
\State \textbf{Input:} Query vector $Q$, result size $k$, distances of candidate neighbors to query $\text{d}_{\text{cache}}$ (sorted in ascending order)
\State \textbf{Output:} True for cache hit, False otherwise
\State $R \gets \textsc{ComputeRegionKey}(Q)$ \label{alg:line:compute-region-key}
\If{$\text{d}_{\text{cache}}[k] \le (1 + D) \cdot \theta[k][R]$} \label{alg:line:cachecheck}
    \State \Return True
\EndIf
\State \Return False
\end{algorithmic}
\end{algorithm}

\subsection{Mini-indexes}
\label{sec:mini-indexes} QVCache organizes its in-memory vectors into multiple mini-indexes, each an instance of a FreshVamana graph~\cite{freshdiskann}, and manages them concurrently. This design narrows the search space during lookups (Algorithm~\ref{alg:cache-search}) and allows each mini-index to be treated as an independent unit for cache eviction.




\textbf{Insertion.}
QVCache maintains eviction metadata to track access patterns (i.e. cache hits, evictions, and insertions) across its mini-indexes, ranking them from hottest to coldest based on the chosen eviction policy. Under the default LRU policy, the hottest mini-index corresponds to the most recently used one. Upon a cache miss, the system attempts to insert the new vector(s) into mini-indexes (Line \ref{alg:line:async-cache-fill} in Algorithm \ref{alg:tiered-search}) in order of decreasing temperature, starting from the hottest and proceeding to progressively colder ones until it finds available space. If no mini-index has free capacity, QVCache evicts the coldest one according to the eviction policy, inserts the vector into that mini-index, and promotes it to be the hottest (similar to line \ref{alg:line:update-lru} in Algorithm \ref{alg:cache-search}). For example, in Figure~\ref{fig:architecture}, the system inserts into Mini-index~2, identified as the hottest according to the eviction policy.

\textbf{Search.} Unlike traditional caches that use hash tables and provide $O(1)$ lookups regardless of cache size, QVCache relies on the FreshVamana~\cite{freshdiskann} graph-based index (\textsc{CacheSearch} in Algorithm \ref{alg:tiered-search}), whose search operation (\textsc{GreadySearch} in \cite{freshdiskann}) converges in logarithmic time, $O(\log N)$, where $N$ is the number of vectors in the index. This implies that increasing cache capacity would increase cache-hit latency, which is undesirable. QVCache mitigates this by leveraging mini-indexes to bound cache lookup cost.


Since QVCache inserts newly cached vectors into the hottest mini-indexes upon a cache miss, queries that produce cache hits are most likely to find their nearest candidates among those same mini-indexes. QVCache exploits this locality by scanning mini-indexes in descending heat order (Line~\ref{alg:line:get-eviction-order} in Algorithm~\ref{alg:cache-search}). Upon detecting a hit in a mini-index, QVCache inserts these candidate neighbors into the candidate set (Lines~\ref{alg:line:check-hit}, \ref{alg:line:add-candidates}). Under the \texttt{EAGER} strategy, the search terminates upon the first hit, whereas the \texttt{SEQUENTIAL} strategy continues scanning all mini-indexes and re-ranks the union of retrieved candidates (Line~\ref{alg:line:rerank}).

While \texttt{SEQUENTIAL} is effective at recovering high-quality candidate sets, its cost grows linearly with the number of mini-indexes, $n_{\text{mini-index}}$. In contrast, \texttt{EAGER} achieves constant-time behavior with respect to $n_{\text{mini-index}}$ in practice; under high hit rates, the first mini-index alone typically suffices, resulting in a search cost of $O(\log c_{\text{mini-index}})$, where $c_{\text{mini-index}}$ is the capacity of a mini-index. To balance these trade-offs, QVCache's \texttt{ADAPTIVE} policy monitors recent hit ratios and switches between \texttt{EAGER} and \texttt{SEQUENTIAL}: when hit rates exceed a threshold (e.g.,~0.9), it adopts \texttt{EAGER}, otherwise it falls back to \texttt{SEQUENTIAL}. We find that \texttt{ADAPTIVE} performs well empirically, though users may select any strategy depending on recall and latency requirements.


\vspace{1ex}

\textbf{Eviction.}
FreshVamana handles deletions in two stages. When a node is marked for deletion, it is first flagged as inactive and excluded from subsequent searches, but its memory remains allocated. Periodically, a background \textit{consolidation} process reclaims memory by removing all flagged nodes and reconnecting the surrounding graph. We observed that this lazy deletion strategy introduces two major drawbacks: (1) the consolidation step requires multi-threaded execution, adding significant overhead ~\cite{turbocharging-vector-databases-ssds}  and reducing throughput for workloads with frequently changing working sets ~\cite{working-set}, and (2) infrequent consolidation can cause memory bloat due to accumulated deleted nodes, which is problematic in memory-constrained environments where QVCache operates.

Because of these limitations, QVCache adopts mini-indexes as the unit of eviction. This enables to avoid costly consolidation operation in FreshVamana \cite{freshdiskann} with the cost of coarser-grained information loss while evicting depending the on the capacity of mini-indexes. When all the mini-indexes get full, it evicts one of them according to eviction policy, e.g. Mini-index 3 in Figure~\ref{fig:architecture}, and promoted to the hottest index for the future insertions to be used. 


%As discussed in Section 6.4, the capacity of each mini-index plays a critical role in performance. If a mini-index is too small, it triggers frequent evictions and fails to exploit the logarithmic convergence properties of FreshVamana~\cite{freshdiskann}. Conversely, overly large mini-indexes increase both query latency and eviction cost i.e. the number of vectors evicted from cache. Ideally, the capacity of each mini-index should approximately match the \textit{working set} size~\cite{working-set}, representing the subset of vectors that are fetched within a reasonably short time window. When allocating additional memory to QVCache, the overall cache size should be scaled by increasing ${\text{n}}_{\text{mini-index}}$ while keeping ${\text{c}}_{\text{mini-index}}$ constant (assuming the working set size remains constant), thereby maintaining stable and efficient latency characteristics.



\subsection{Cache Hit and Miss Decisions}
Deciding if a vector search query can be answered from cache (Algorithm~\ref{alg:is-hit}) is a similarity caching problem~\cite{similarity-caching, similarity-caching-theory-algorithms}. A query is considered a cache hit if the distance of the query vector to the furthest vector,$d_{\text{cache}}[k]$, in the candidate neighbor set is less than or equal to a \textit{similarity threshold}, $\theta[k][R]$. Determining the optimal threshold to maximize hit ratios while maintaining the recall of the backend database is challenging. If the threshold is too small, cache misses occur too frequently. If it is too large, cache hits return very different results from the cache, resulting in low recalls. We list four key challenges around the cache hit and miss decisions and respective solutions we propose.

\vspace{1ex}
 \textbf{Challenge 1: No universal threshold across datasets.} Different datasets may require substantially different distance thresholds, making manual tuning impractical.  

 \textit{Solution: Learned Thresholds.} QVCache infers distance thresholds based on cache misses (Algorithm~\ref{alg:learn-threshold}), thereby automatically adapting these thresholds across different datasets.
 

\vspace{1ex}
 \textbf{Challenge 2: No universal threshold across data regions.} Vectors in a database are distributed non-uniformly across the high-dimensional space. Some regions are dense and highly clustered, while others are sparse, and certain regions contain more vectors than others. Consequently, a single global similarity threshold may perform well for some queries but poorly for others, even though the dataset is the same.

 \textit{Solution: Spatial thresholds.} It partitions the high-dimensional space into sub-regions (sub-spaces) and assigns a separate similarity threshold to each. These thresholds are learned independently, and for a given query, QVCache uses the threshold corresponding to the sub-region (sub-space) onto which the query vector falls to make cache hit decisions.

 %\begin{figure}[h]
 % \centering
 % \begin{minipage}[t]{0.48\linewidth}
 %   \centering
 %   \includegraphics[width=\linewidth]{figures/first.pdf}% first image
 % \end{minipage}
 % \hfill
 % \begin{minipage}[t]{0.48\linewidth}
 %   \centering
 %   \includegraphics[width=\linewidth]{figures/second.pdf}% second image
 % \end{minipage}
 % \caption{Initial queries help QVCache learn spatial similarity thresholds (left), and subsequent queries falling into these regions are classified as cache hits or misses (right). Blue dots represent data vectors, green dots queries resulting in cache hits, and red dots queries resulting in cache misses.}
 % \label{fig:spatial-thresholds}
%\end{figure}

\vspace{1ex}
    \textbf{Challenge 3: No universal thresholds over time.} Even within the same sub-region, query patterns evolve, so the threshold that yields the best cache hit/miss decisions may change over time.
    
    \textit{Solution: Continuous learning.} QVCache runs Algorithm~\ref{alg:learn-threshold} continuously to adapt its thresholds, allowing it to track changes in query patterns.

\vspace{1ex}
 \textbf{Challenge 4: No universal threshold across different $k$ values.} As the parameter $k$ in a vector search query increases, the system generally requires a higher threshold. However, the threshold for a larger $k$ (e.g., $k=10$) cannot be inferred from that of a smaller $k$ (e.g., $k=1$), as the relationship is highly nonlinear and often unpredictable across datasets.

 \textit{Solution: $k$-dependent Thresholds.} QVCache maintains an independent threshold for each observed $k$ and learns them separately.

\vspace{1ex}
To implement these solutions, QVCache maintains a 2D array of thresholds, $\theta$. As shown in Algorithms ~\ref{alg:is-hit} and ~\ref{alg:learn-threshold} , the first dimension is resolved by the parameter $k$, while the second corresponds to the region $R$ into which the query falls, which is computed by \textsc{ComputeRegionKey} and described in detail in Section~\ref{sec:method:spatial-thresholding}. When evaluating whether a query is a cache hit, the system does not directly compare the distance of the furthest vector in the candidate set, $\text{d}_{\text{cache}}[k]$, with the threshold. Instead, it applies a multiplicative adjustment using the \textit{deviation factor}, $D$ (Line \ref{alg:line:cachecheck}). This factor serves as a tunable knob that balances cache hit ratio and recall: increasing $D$ raises the hit ratio but may lead to reduced recall. Such a knob provides flexibility for users to adapt QVCache to systems with different accuracy and performance requirements beyond the learned thresholds. In our experiments, we found that setting $D$ in the range $[0, 0.5]$ provides a practical operating regime.


\begin{algorithm}[htbp]
\caption{\textsc{LearnThreshold}}
\label{alg:learn-threshold}
\begin{algorithmic}[1]
\State \textbf{Input:} Query vector $Q$, result size $k$, distances of candidate neighbors to query $\text{d}_{\text{backend}}$ (sorted in ascending order)
\State $R \gets \textsc{ComputeRegionKey}(Q)$
\State $\theta[k][R] \gets (1 - \alpha) \cdot \theta[k][R] + \alpha \cdot \text{d}_{\text{backend}}[k]$ \label{alg:line:updatetheta}
\end{algorithmic}
\end{algorithm}

  
\subsection{Learning Thresholds}
In  Algorithm~\ref{alg:is-hit}, QVCache uses the distance of the furthest vector in the candidate set generated by itself, $\text{d}_{\text{cache}}[k]$, to determine the cache hit/miss. On the other hand, Algorithm~\ref{alg:learn-threshold} uses the distance of the furthest vector returned by the backend, $\text{d}_{\text{backend}}[k]$ to learn the spatial thresholds in case of cache misses,  $\theta[k][R]$ in  Algorithm~\ref{alg:is-hit}. Ideally, $\text{d}_{\text{cache}}[k]$ converges to $\text{d}_{\text{backend}}[k]$ for a cache hit. 

The threshold update mechanism in QVCache was inspired by the Adam optimizer~\cite{adamoptimizer}, which updates momentum using gradients during neural network training. Similarly, QVCache updates thresholds using feedback from backend query results. The update employs an \textit{adaptivity rate}, $\alpha$, which determines how quickly QVCache adapts to changes in the query distribution (Line \ref{alg:line:updatetheta} in Algorithm \ref{alg:learn-threshold}). 

The first term in the update equation preserves the momentum of past query behavior, while the second term enables adaptation to query distribution shifts. A higher adaptivity rate, $\alpha$, allows QVCache to adjust more quickly to such shifts but also causes it to forget past query patterns faster. This balance enables QVCache to remain both stable and responsive under dynamic workloads.
In our experiments, we found that setting $\alpha = 0.9$ provides a good trade-off between stability and responsiveness.

%For each query resulting in a cache miss, QVCache populates the cache by fetching the top-$k$ vectors from the backend by their IDs and inserting them into the appropriate mini-indexes (Section \ref{sec:mini-indexes}), while also updating the thresholds using Algorithm~\ref{alg:learn-threshold}. To avoid using stale index data that could degrade recall, threshold updates are performed only after corresponding top-$k$ vectors are inserted into mini-indexes. Fetching vectors from the backend can be costly depending on the deployment strategy (i.e. whether QVCache and the backend are co-located or the backend is remote) and the backend's access path (i.e. whether it reads vectors from disk or keeps them in memory) to vectors. QVCache avoids blocking the critical query path, minimizing latency by executing both the cache and threshold updates asynchronously through a pool of background threads, as described in Algorithm~\ref{alg:tiered-search}.

%Due to the asynchronous execution of these tasks, if QVCache receives a query before the completion of cache update triggered by previous similar queries, it may result in a cache miss. This occurs because the thresholds used during this period are still stale, even though the query would have been a cache hit after the update is applied.

\subsection{Scalable Spatial Thresholding in High Dimensions}\label{sec:method:spatial-thresholding}
QVCache partitions the high-dimensional vector space into sub-spaces by dividing each dimension of the vector space into ${\text{n}}_{\text{buckets}}$ buckets. Upon receiving a query, it identifies the bucket corresponding to each dimension (\textsc{ComputeRegionKey}) to determine the appropriate spatial threshold.

However, pre-allocating the buckets for all possible sub-spaces is infeasible, especially for high-dimensional vectors.
For example, SIFT dataset vectors \cite{sift} have 128 dimensions, where storing the thresholds for all possible sub-spaces requires $128^{16}$ thresholds for ${\text{n}}_{\text{buckets}}$=16. 

\textbf{Dimensionality reduction.} Queries that are close in the original high-dimensional space remain close after projection into a lower-dimensional space, so the proximity relationships relevant for cache-hit decisions are approximately preserved. Therefore, QVCache projects incoming query vectors onto a lower-dimensional space via Principal Component Analysis (PCA) \cite{principalcomponentanalysis} and uses this reduced space for both threshold assignment and learning. QVCache performs a lightweight offline training step to compute the PCA transformation matrix, and this process does not require access to the full dataset; in our experiments, random sampling of as little as 0.1\%–0.01\% of the dataset was sufficient.

On top of the projection, QVCache applies a straightforward partitioning scheme, dividing each dimension of the reduced space into equal-sized buckets. More advanced partitioning strategies that account for dataset-specific characteristics could be explored as future work. The \textsc{ComputeRegionKey} function applies the learned PCA transformation to project each query into the reduced space and identifies its corresponding region by locating the buckets it falls into.



For instance, if ${\text{d}}_{\text{reduced}} = 8$, the memory requirement becomes $8^{16}$ floating points for ${\text{n}}_{\text{buckets}}$=16. Although this technique significantly reduces the memory footprint, it still exceeds the practical constraints under which QVCache is designed to operate.

\textbf{Lazy initialization.} Similar to how data vectors tend to cluster, queries also exhibit spatial locality, meaning that not every region of the vector space will receive queries. Consequently, QVCache does not allocate memory for thresholds in regions that have not yet been queried; a missing key implicitly represents a region which has not been queried yet. Memory is allocated only when the first query for a new region arrives and the threshold is initialized to $d_{backend}[k]$ (i.e. updating the threshold by setting $\alpha$ to 1 in Algorithm \ref{alg:learn-threshold}). Our experiments show that, at most 50,000 regions (using the SpaceV  dataset \cite{SPACEV1B_SPTAG} with ${\text{d}}_{\text{reduced}} = 16$ and ${\text{n}}_{\text{buckets}}$=16) become active, consuming approximately 200~KB of memory. If the number of active regions exceeds a user-defined limit, QVCache can evict thresholds (by resetting them to zero and freeing memory) using an eviction policy such as LRU, and re-learn them later as needed. This lazy initialization, combined with dimensionality reduction, allows QVCache to maintain an exceptionally low memory footprint.




\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{plots/noise_ratio_analysis/neighbor_overlap_analysis.pdf}
    \caption{\textbf{Nearest Neighbor overlap under perturbation (DEEP \cite{deep} dataset, $k=10$).} The overlap between the neighbor sets of the base and perturbed queries decays sharply, approaching near-zero at a noise ratio of 0.5.}
    \label{fig:overlap-analysis}
\end{figure}


\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{figures/query_perturbation}
        \caption{Synthesizing Queries with Semantic (Spatial) Locality}
        \label{fig:query-perturbation}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \resizebox{0.8\textwidth}{!}{
            \input{figures/sliding_window_experiment}
        }
        \caption{Workload Generation with temporal-semantic Locality}
        \label{fig:sliding-window}
    \end{subfigure}
    \caption{Evaluation framework proposed in this paper for benchmarking vector caches, used to evaluate QVCache.}
    \label{fig:evaluation-framework}
\end{figure*}