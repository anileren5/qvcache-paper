\section{Related Work}

\textbf{Vector Databases:} The growing demand for managing embedding data has led to the development of numerous vector database management systems in recent years \cite{pinecone, pgvector, ZillizServerless2025, milvus, qdrant2025, OpenSearch, 10.14778/3685800.3685806-gaussdb}. These systems incorporate a variety of optimizations tailored to vector data, including storage architectures, lock management, and query processing. They play a crucial role in Retrieval-Augmented Generation (RAG) by efficiently retrieving relevant documents or embeddings that provide additional context, which improves the factual accuracy and reasoning capabilities of Large Language Models \cite{Jiang_2025, gao2024retrievalaugmentedgenerationlargelanguage}. Beyond RAG, vector databases are also used in recommendation systems \cite{Covington:2016:DNN:YouTube, 10.1145/3035918.3064009, shen2024learningretrievejobmatching} and semantic search engines \cite{veluru2025evolution}. Moreover, several studies have explored accelerating ANN search using specialized hardware, including GPUs~\cite{bang,scalablegraphindexingusinggpus} and FPGAs~\cite{vector-search-delayed-fpga}. Furthermore, as vector data is increasingly combined with relational data, recent research has focused on supporting fundamental relational operations such as joins and filtering within vector databases, which are known as similarity joins \cite{Chen_2025} and filtered vector search \cite{10.14778/3750601.3750700}.

\textbf{Vector Search Indexes:} Vector databases employ a variety of index structures to optimize performance while maintaining high recall in Approximate Nearest Neighbor (ANN) search. These structures include graph-based indexes \cite{diskann, HNSWMalkovY16, nsg, 9383170}, which organize vectors into navigable graphs for efficient neighbor exploration; tree-based indexes \cite{10.14778/3204028.3204034}, particularly effective for low-dimensional data; hash-based approaches \cite{locality-sensitive-hashing, jafari2021surveylocalitysensitivehashing}, which leverage hash functions to map similar vectors to the same buckets; and inverted indexes \cite{product-quantization, Lempitsky2015TheIM}, which partition the vector space into clusters and use quantization techniques to accelerate search while reducing memory usage.

Initially, vector search was primarily performed over static collections, where the entire dataset was known in advance and the index could be built once. However, modern applications such as recommendation systems, online advertising, and real-time analytics require continuously evolving datasets. This has created a need for dynamic vector search indexes that can efficiently support the insertion of new vectors and the deletion of existing ones without rebuilding the entire index, since full rebuilds are computationally expensive and impractical for large-scale or streaming data. To address this, several dynamic indexing approaches have been developed \cite{mohoney2025quakeadaptiveindexingvector, xu2025inplaceupdatesgraphindex}, which aim to maintain high recall and query performance while performing updates incrementally. The main challenges they tackle include preserving the index structure quality, minimizing degradation of search accuracy, and ensuring efficient update operations in streaming or rapidly changing workloads.


\textbf{Caching in Vector Databases:} Caching in the context of vector databases \cite{jeong2025callcontextawarelowlatencyretrieval, milvus, 10.14778/3685800.3685805, turbocharging-vector-databases-ssds, diskann, tiered-cache-hnsw} primarily focuses on optimizing data access patterns so that the random access behavior inherent in vector search results in minimal page cache misses from the perspective of the operating system or database page cache. Rather than attempting to completely eliminate disk I/O, these approaches aim to reduce it by batching similar queries, reordering queries to improve locality, building the index in an I/O-friendly layout, or keeping frequently accessed portions of the index in memory.


\textbf{Similarity Caching:} Similarity caching is a well-studied theoretical problem \cite{similarity-caching, similarity-caching-theory-algorithms}. In recent years, it has found applications across various domains. One common approach is semantic caching, where previously cached results are used to serve similar prompts to large language models (LLMs) in retrieval-augmented generation (RAG) pipelines or conversational chat systems \cite{10.14778/3750601.3750679, 10.1145/3578519, 10.14778/3685800.3685905, 10.1145/3721146.3721941}. Additionally, a recent study proposed a method for semantic caching that provides formal guarantees on the error rate \cite{vcache}.
