\section{Related Work}

\textbf{Vector Databases:} The growing demand for managing embedding data has led to the development of numerous vector database management systems in recent years \cite{pinecone, pgvector, ZillizServerless2025, milvus, qdrant2025, OpenSearch, 10.14778/3685800.3685806-gaussdb}. These systems incorporate a variety of optimizations tailored to vector data, including storage architectures, lock management, and query processing. Moreover, several studies have explored accelerating ANN search using specialized hardware, including GPUs~\cite{bang,scalablegraphindexingusinggpus} and FPGAs~\cite{vector-search-delayed-fpga}. Furthermore, as vector data is increasingly combined with relational data, recent research has focused on supporting fundamental relational operations such as joins and filtering within vector databases, which are known as similarity joins \cite{Chen_2025} and filtered vector search \cite{10.14778/3750601.3750700}.

%\textbf{Vector Search Indexes:} Vector databases employ a variety of index structures to optimize performance while maintaining high recall in Approximate Nearest Neighbor (ANN) search. These structures include graph-based indexes \cite{diskann, HNSWMalkovY16, nsg, 9383170}, which organize vectors into navigable graphs for efficient neighbor exploration; tree-based indexes \cite{10.14778/3204028.3204034}, particularly effective for low-dimensional data; hash-based approaches \cite{locality-sensitive-hashing, jafari2021surveylocalitysensitivehashing}, which leverage hash functions to map similar vectors to the same buckets; and inverted indexes \cite{product-quantization, Lempitsky2015TheIM}, which partition the vector space into clusters and use quantization techniques to accelerate search while reducing memory usage.

%Initially, vector search was primarily performed over static collections, where the entire dataset was known in advance and the index could be built once. However, modern applications such as recommendation systems, online advertising, and real-time analytics require continuously evolving datasets. This has created a need for dynamic vector search indexes that can efficiently support the insertion of new vectors and the deletion of existing ones without rebuilding the entire index, since full rebuilds are computationally expensive and impractical for large-scale or streaming data. To address this, several dynamic indexing approaches have been developed \cite{mohoney2025quakeadaptiveindexingvector, xu2025inplaceupdatesgraphindex}, which aim to maintain high recall and query performance while performing updates incrementally. The main challenges they tackle include preserving the index structure quality, minimizing degradation of search accuracy, and ensuring efficient update operations in streaming or rapidly changing workloads.


\textbf{Caching in Vector Databases:}  Caching in vector databases typically refers to system-level mechanisms \cite{jeong2025callcontextawarelowlatencyretrieval, milvus, 10.14778/3685800.3685805, turbocharging-vector-databases-ssds, diskann, tiered-cache-hnsw} that are tightly coupled with their respective systems and underlying indexes. These approaches primarily aim to reduce the cost of disk accesses arising from random I/O during graph traversal. For example, \cite{diskann} caches nodes near traversal entry points in memory, \cite{tiered-cache-hnsw} keeps the uppermost HNSW layer resident in memory, \cite{jeong2025callcontextawarelowlatencyretrieval} batches queries by aligning their page requests, and \cite{turbocharging-vector-databases-ssds} reorganizes the on-disk index layout to minimize page-cache misses.


\textbf{Similarity Caching:}
Similarity based caching has recently gained traction in the context of Large Language Model (LLM) APIs and document retrieval. The core idea is to place a query level cache in front of the model or retrieval engine: if an incoming prompt or query is sufficiently similar to a previously seen one, according to a similarity function and a predetermined threshold, the response is returned directly from the cache. Because conversational systems and RAG pipelines often exhibit substantial semantic repetition across queries \cite{10.14778/3750601.3750679, 10.1145/3578519, 10.14778/3685800.3685905, 10.1145{3721146.3721941}, gptcache}, this strategy has proven highly effective. For example, \cite{gptcache} stores LLM prompts and responses to eliminate redundant API calls, while \cite{10.14778/3685800.3685905} caches retrieved document sets to accelerate subsequent retrieval queries. Additionally, a recent study \cite{vcache} proposed a method for semantic caching that provides formal guarantees on the error rate.


