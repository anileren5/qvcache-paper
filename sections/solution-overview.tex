\section{Solution Overview}
We introduce \textbf{QVCache}, a query-level cache for vector search that opportunistically bypasses backend ANN execution when it can return recall-valid results using vectors retrieved from prior cache misses. QVCache is designed for workloads exhibiting \emph{temporal-semantic locality}, where queries recur within short time windows as nearby points in the embedding space, even though exact query vectors rarely repeat.
% Building on these observations, we introduce QVCache, the first (to our knowledge) query-level vector cache that bypasses the backend database when it can confidently answer a query using vectors previously retrieved on cache misses. QVCache is designed for workloads exhibiting temporal-semantic locality, where queries recur within a short time window and in semantically similar (i.e., spatially closer) forms. 

QVCache operates as a transparent layer in front of an existing vector database. It neither modifies backend index structures nor depends on backend-specific heuristics. Instead, it exploits workload locality to amortize expensive ANN execution across semantically similar queries, while bounding cache memory usage and lookup latency independently of dataset size.

\subsection{Workload Characteristics}  
\label{sec:workload-characteristics}
Traditional caching relies on temporal locality alone, assuming that identical requests recur. Vector search workloads violate this assumption: even minor variations in phrasing or prompts yield distinct embeddings, rendering exact-match caching ineffective. However, production workloads often exhibit a stronger property that we term \emph{temporal-semantic locality}: queries that are close in time are frequently close in embedding space.

This behavior has been observed in e-commerce search, where multiple formulations reflect the same purchase intent, and in conversational and RAG workloads, where users iteratively refine prompts. Prior studies further show that temporal access patterns correlate with semantic similarity in real search workloads. Because semantically similar queries tend to retrieve highly overlapping nearest-neighbor sets, caching vectors retrieved by one query can accelerate subsequent, nearby queries without degrading recall.

Empirical evidence from industrial systems reinforces this observation; in large-scale IVF-based deployments, only about 15\% of IVF partitions are accessed over an entire day~\cite{incrementalivfindexmaintenance}, and the active working set over cache-relevant timescales (seconds to minutes) is often well below 1\% of the dataset. This pronounced access skew makes query-level vector caching practical and effective. By maintaining a compact in-memory hot set, QVCache can serve cache hits with orders-of-magnitude lower latency while keeping memory usage strictly bounded.
%Unlike traditional caching, which relies solely on temporal locality, we coin the term \textit{temporal-semantic locality} to describe the behavior where semantically similar (i.e. spatially closer) queries repeat over time. This pattern is especially prevalent in e-commerce search~\cite{semanticequivalenceecommercequeries} and chatbot systems~\cite{gptcache}, where different phrasings reflect the same intent and yield nearby embeddings. Prior work further indicates that temporal access patterns correlate with semantic similarity in search workloads~\cite{semantic-similarity-temporal-correlation}. Because such queries often retrieve overlapping nearest-neighbor sets, QVCache can maintain a compact hot set of vectors in memory and efficiently serve repeated, similar queries.

%In an industrial vector search workload, production evidence shows that only about 15\% of IVF partitions are accessed over an entire day~\cite{incrementalivfindexmaintenance}. At the short timescales relevant for caching, for example minutes or even seconds, the active portion of the dataset is likely far below 1\%. This access skew makes vector caching highly practical. Leveraging these properties, QVCache achieves orders of magnitude lower latency (up to 300×) and corresponding throughput gains on a cache hit, while maintaining a minimal and bounded memory footprint.

\begin{figure}[!htb]
  \centering
  \begin{subfigure}[b]{0.6\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/hit.pdf}
    \caption{Cache hit}
    \label{fig:hit}
  \end{subfigure}
  \hspace{0.04\linewidth}
  \begin{subfigure}[b]{0.6\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/miss.pdf}
    \caption{Cache miss}
    \label{fig:miss}
  \end{subfigure}
  \caption{Handling cache hits and misses in QVCache with four mini-indexes, each with a capacity of three vectors, using an LRU eviction policy.}
  \label{fig:architecture}
\end{figure}

\subsection{Query Flow through QVCache}  
QVCache processes every incoming query before it reaches the backend database. It first performs a \emph{cache search} over a collection of in-memory mini-indexes, each implemented as a small dynamic ANN graph i.e. a FreshVamana index. The cache search produces a candidate set of $k$ nearest neighbors.
%QVCache sits in front of a vector database of the user's choice, referred to as the \textit{backend database}. Incoming queries are first processed by QVCache, which searches through its in-memory mini-indexes. During this initial \textit{cache search}, QVCache generates a candidate set of $k$ nearest neighbors as specified by the query. 

To determine whether this candidate set is sufficiently accurate, QVCache compares the distance of the $k$-th neighbor against a \emph{region-specific similarity threshold}. If the distance falls within the threshold, the query is classified as a cache hit and answered directly from the cache, bypassing backend execution. These thresholds are not fixed globally; they are learned online from observed backend responses and adapt to local distance distributions in the embedding space, allowing QVCache to preserve recall while avoiding overly conservative cache decisions.
%If it determines that this candidate set is sufficiently accurate with respect to the unknown ground truth, the query is marked as a cache hit and answered directly from the cache, without contacting the backend database. The cache-hit decision is based on comparing the distance of the $k$-th neighbor in the candidate set to a region-specific \textit{similarity (distance) threshold}, which is dynamically learned over time from cache misses.

On a cache miss, the query is forwarded to the backend ANN system. Once the backend returns the result, QVCache fetches the corresponding vectors for the returned neighbor IDs and inserts them into the cache. Insertions are performed into the currently active mini-index, selected according to the cache’s eviction policy metadata (e.g., most recently used under an LRU policy). After cache population completes, QVCache updates the similarity thresholds using an online learning procedure driven by the newly observed nearest-neighbor distances.
%When a query results in a cache miss, it is redirected to the backend database for resolution. Upon receiving the response, QVCache populates its mini-indexes by fetching the corresponding $k$ vectors associated with the returned neighbor IDs. Afterwards, it updates the spatial thresholds using an online learning procedure. The fetched vectors are inserted into the hottest mini-index according to the eviction policy metadata (e.g., the most recently used mini-index in an LRU policy). 

Both vector fetching and threshold updates are performed asynchronously to keep cache-miss latency on the critical path identical to backend latency. This is essential in disk-based or remote deployments, where fetching vectors can incur multiple random I/O operations or network transfers. To avoid using stale statistics, thresholds are updated only after cache insertion completes. As a result, vectors retrieved on a miss become available to future queries with a short delay, but never compromise recall.
%Both cache population and threshold updates are performed asynchronously to avoid increasing latency along the query's critical path, as fetching vectors from the backend can incur multiple random disk accesses ~\cite{turbocharging-vector-databases-ssds} or network transfers if QVCache is deployed on a separate machine. Threshold updates occur after the cache-fill completes to prevent using stale data, which could otherwise degrade recall. Consequently, vectors from a cache miss are not immediately available in the cache but become accessible with some delay.

QVCache initializes empty and fills its mini-indexes upon cache misses. Memory usage is bounded by construction: each mini-index has a fixed capacity, and eviction occurs at the granularity of entire mini-indexes rather than individual vectors. This design avoids fine-grained deletions, simplifies concurrency control, and ensures predictable memory overhead. The evicted mini-index is selected according to the configured policy, such as the least recently used mini-index illustrated in Figure~\ref{fig:architecture}.
%To bound memory usage, each mini-index has an upper limit on the number of vectors it can store, and eviction is performed at the mini-index level rather than for individual vectors. The appropriate mini-index is evicted according to the configured policy metadata, such as the least recently used mini-index illustrated in Figure~\ref{fig:architecture}.

By short-circuiting backend ANN execution on cache hits, QVCache substantially reduces query latency and backend load. In cloud-based deployments where vector search is billed per query~\cite{ZillizServerless2025}, this reduction directly translates into lower operational cost, particularly at large scale.
%Thanks to this architecture, QVCache achieves a significant reduction in query latency by serving queries directly from the cache on a hit. The high cache-hit rate also translates into lower operational costs for cloud-based systems, where billing is often charged per query~\cite{ZillizServerless2025}, which can become expensive for large datasets.
