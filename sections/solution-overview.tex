\section{Solution Overview}
Building on these observations, we introduce QVCache, the first (to our knowledge) query-level vector cache that bypasses the backend database when it can confidently answer a query using vectors previously retrieved on cache misses. QVCache is designed for workloads exhibiting temporal-semantic locality, where queries recur not only over time but also in semantically similar (i.e. spatially closer) forms. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{figures/cache-miss.pdf}
  \caption{Illustration of cache-miss handling in a configuration with four mini-indexes, each with a capacity of three vectors.}
  \label{fig:architecture}
\end{figure}

\subsection{Workload Characteristics}  
Unlike traditional caching, which relies solely on temporal locality, we coin the term \textit{temporal-semantic locality} to describe the behavior where semantically similar (i.e. spatially closer) queries repeat over time. This pattern is especially prevalent in e-commerce search~\cite{semanticequivalenceecommercequeries} and chatbot systems~\cite{gptcache}, where different phrasings reflect the same intent and yield nearby embeddings. Prior work further indicates that temporal access patterns correlate with semantic similarity in search workloads~\cite{semantic-similarity-temporal-correlation}. Because such queries often retrieve overlapping nearest-neighbor sets, QVCache can maintain a compact hot set of vectors in memory and efficiently serve repeated, similar queries.

In an industrial vector search workload, production evidence shows that only about 15\% of IVF partitions are accessed over an entire day~\cite{incrementalivfindexmaintenance}. At the short timescales relevant for caching, for example minutes or even seconds, the active portion of the dataset is likely far below 1\%. This access skew makes vector caching highly practical. Leveraging these properties, QVCache achieves orders of magnitude lower latency (up to 300Ã—) and corresponding throughput gains on a cache hit, while maintaining a minimal and bounded memory footprint.

\subsection{Query Flow through QVCache}  
QVCache sits in front of a vector database of the user's choice, referred to as the \textit{backend database}. Incoming queries are first processed by QVCache, which searches through its in-memory mini-indexes. During this initial \textit{cache search}, QVCache generates a candidate set of $k$ nearest neighbors as specified by the query. If it determines that this candidate set is sufficiently accurate with respect to the unknown ground truth, the query is marked as a cache hit and answered directly from the cache, without contacting the backend database. The cache-hit decision is based on comparing the distance of the $k$-th neighbor in the candidate set to a region-specific \textit{similarity (distance) threshold}, which is dynamically learned over time from cache misses.

When a query results in a cache miss, it is redirected to the backend database for resolution. Upon receiving the response, QVCache populates its mini-indexes by fetching the corresponding $k$ vectors associated with the returned neighbor IDs. Afterwards, it updates the spatial thresholds using an online learning procedure. The fetched vectors are inserted into the hottest mini-index according to the eviction policy metadata (e.g., the most recently used mini-index in an LRU policy). Both cache population and threshold updates are performed asynchronously to avoid increasing latency along the query's critical path, as fetching vectors from the backend can incur multiple random disk accesses ~\cite{turbocharging-vector-databases-ssds} or network transfers if QVCache is deployed on a separate machine. Threshold updates occur after the cache-fill completes to prevent using stale data, which could otherwise degrade recall. Consequently, vectors from a cache miss are not immediately available in the cache but become accessible with some delay.

QVCache initializes empty and fills its mini-indexes upon cache misses. To bound memory usage, each mini-index has an upper limit on the number of vectors it can store, and eviction is performed at the mini-index level rather than for individual vectors. The appropriate mini-index is evicted according to the configured policy metadata, such as the least recently used mini-index illustrated in Figure~\ref{fig:architecture}.

Thanks to this architecture, QVCache achieves a significant reduction in query latency by serving queries directly from the cache on a hit. The high cache-hit rate also translates into lower operational costs for cloud-based systems, where billing is often charged per query~\cite{ZillizServerless2025}, which can become expensive for large datasets.
